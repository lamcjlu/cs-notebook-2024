{
 "cells": [
  {
   "cell_type": "code",
   "id": "f1e61055-ed63-4ccd-ad37-f58dbfe28483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T11:17:45.318338Z",
     "start_time": "2024-05-14T11:17:34.348922Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "#from pettingzoo.classic import tictactoe_v3\n",
    "from typing import List\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#import ipyvolume as ipv\n",
    "from functools import reduce\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from torch import autocast\n",
    "from torch.distributions import Categorical\n",
    "#from torchrl.envs.libs import pettingzoo\n",
    "\n",
    "torch.set_default_dtype(torch.float)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA IS TRUE\", torch.get_autocast_gpu_dtype)\n",
    "    torch.set_default_device('cuda')\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, size=(21, 21, 21), diff=1, risk_prob=0.1, death_prob=0.01):\n",
    "        if torch.cuda.is_available():\n",
    "          device = 'cuda'\n",
    "        self.size = size  # Grid size, e.g., (100, 100, 100) for 3D\n",
    "        self.vol = size[0] * size[1] * size[2]\n",
    "        self.diff = diff  # A difficulty or variability factor\n",
    "        self.dim = len(size)\n",
    "        self.target = (0, 0, 0, 0)  # energy-p, wall-p, risk-p, death-p by volume or by cells, index by 0\n",
    "        self.isRiskCluster = True\n",
    "\n",
    "        # main game -> min (time(steps) * total_energy) -> compared with (l2_distance * harmonic_or_mean_or_70_or_max_energy). if agent_score < estimated_score, good. if a_s > e_s, improve. hm7m is param tightened by agent performance\n",
    "        # minimize energy = main objective\n",
    "        self.energy = torch.clamp(torch.normal(0.2, 0.5, size=self.size), min=0, max=3)\n",
    "        # cannot transpose to wall, if slip into wall -> terminate , death ,-r\n",
    "        self.wall = torch.zeros(size)\n",
    "        # risk will lead to slip to a random neighbor cell\n",
    "        self.risk = torch.mul(torch.rand(size), risk_prob)\n",
    "        # if death will terminal state, negative reward (-r)\n",
    "        self.death = torch.mul(self.risk, death_prob)\n",
    "\n",
    "        self.init_properties()\n",
    "        # map = not wall(death + energy * risk)) // Energy has been normalised to 0-1\n",
    "\n",
    "        self.map = torch.add(\n",
    "            torch.mul(\n",
    "                (self.energy * (1 + self.risk) / self.energy.max()),\n",
    "                torch.logical_not(self.wall)\n",
    "            ),\n",
    "            self.death)\n",
    "\n",
    "    def set_diff(self):\n",
    "        print(self.vol)\n",
    "        if self.diff == 1:\n",
    "            self.target = (1, self.vol * 0.1, 0.1, 0.01)\n",
    "            print(\"Setting Diff: \", self.diff, \" -> \", self.target)\n",
    "            return\n",
    "\n",
    "        if self.diff == 2:\n",
    "            self.target = (1.2, self.vol * 0.2, 0.2, 0.05)\n",
    "            print(\"Setting Diff: \", self.diff, \" -> \", self.target)\n",
    "            return\n",
    "\n",
    "        if self.diff == 3:\n",
    "            self.target = (2, self.vol * 0.3, 0.3, 0.1)\n",
    "            print(\"Setting Diff: \", self.diff, \" -> \", self.target)\n",
    "            return\n",
    "\n",
    "    def init_properties(self):\n",
    "        # Initialize grids with normal distributions\n",
    "        self.diff = np.random.randint(1, 4)\n",
    "        self.set_diff()\n",
    "        volume = reduce(operator.mul, self.size, 1)\n",
    "        print(\"INIT: Volume\", volume)\n",
    "        print(\"INIT: target: \", self.target)\n",
    "        mask_r = int(max(self.size) * 0.1)  # 70% limit of any dim\n",
    "\n",
    "        def create_hot_tensor(shape, min_value, max_value):\n",
    "            dims = len(shape)\n",
    "            ranges = [torch.arange(s, dtype=torch.float16) - (s - 1) / 2.0 for s in shape]\n",
    "            grid = torch.meshgrid(ranges, indexing='ij')\n",
    "            dist_matrix = torch.stack([torch.abs(grid[dim]) for dim in range(dims)]).max(0).values\n",
    "            steps = max((s - 1) / 2 for s in shape)\n",
    "            increment = (max_value - min_value) / steps if steps != 0 else 0\n",
    "            values = min_value + (steps - dist_matrix) * increment\n",
    "            values = torch.clamp(values, min=min_value, max=max_value).int()  # Ensure values are within specified range\n",
    "            return values\n",
    "\n",
    "        def set_random_dim_to_one(size):\n",
    "            return tuple(np.where(np.arange(len(size)) == np.random.randint(len(size)), 2, size))\n",
    "\n",
    "        def apply_hot_region(tensor, center, size, min_value, max_value, mode):\n",
    "            if mode == 0:\n",
    "                hot_tensor = create_hot_tensor(size, min_value, max_value)\n",
    "            elif mode == 1:\n",
    "                size = set_random_dim_to_one(size)  # shrink size by x dim\n",
    "                hot_tensor = torch.ones(size)\n",
    "\n",
    "            # Calculate slice for each dimension for the parent tensor\n",
    "            parent_slices = []\n",
    "            hot_tensor_slices = []\n",
    "\n",
    "            for c, s, es in zip(center, size, tensor.shape):\n",
    "                # Start and end points for the slice on the parent tensor\n",
    "                start = max(0, c - s // 2)\n",
    "                end = min(c + (s + 1) // 2, es)\n",
    "\n",
    "                # Corresponding start and end points on the hot_tensor\n",
    "                hot_start = max(0, s // 2 - c if c < s // 2 else 0)\n",
    "                hot_end = s - max(0, (c + (s + 1) // 2) - es)\n",
    "\n",
    "                # Append the slices to the lists\n",
    "                parent_slices.append(slice(start, end))\n",
    "                hot_tensor_slices.append(slice(hot_start, hot_end))\n",
    "\n",
    "            # Convert lists to tuples\n",
    "            parent_slices = tuple(parent_slices)\n",
    "            hot_tensor_slices = tuple(hot_tensor_slices)\n",
    "\n",
    "            # Place the correctly sliced hot_tensor into the parent tensor\n",
    "            tensor[parent_slices] = hot_tensor[hot_tensor_slices]\n",
    "            return tensor\n",
    "\n",
    "        def set_tensor_edges_to_one(tensor):\n",
    "            \"\"\"\n",
    "            Sets the edges of an n-dimensional tensor to 1.\n",
    "\n",
    "            Args:\n",
    "            - tensor (torch.Tensor): An n-dimensional tensor.\n",
    "\n",
    "            Returns:\n",
    "            - torch.Tensor: The modified tensor with its edges set to 1.\n",
    "            \"\"\"\n",
    "            # Iterate over each dimension and set the edge indices to 1\n",
    "            for dim in range(tensor.ndim):\n",
    "                # Get a list of all slice(None) initially which means select everything along each dimension\n",
    "                indexer = [slice(None)] * tensor.ndim\n",
    "\n",
    "                # Set the first and last index of the current dimension to 1\n",
    "                indexer[dim] = 0\n",
    "                tensor[tuple(indexer)] = 1\n",
    "                indexer[dim] = -1\n",
    "                tensor[tuple(indexer)] = 1\n",
    "\n",
    "            return tensor\n",
    "\n",
    "        def rand_center(tensor, r):\n",
    "            # Ensure that the generated center is at least `r` away from the edges of the tensor\n",
    "            return tuple(np.random.randint(low=r, high=dim - r) if dim > 2 * r else r for dim in tensor.shape)\n",
    "\n",
    "        def tensor_vol(dim, vol):\n",
    "            k = 1\n",
    "            random_matrix = torch.clamp(torch.rand(dim), max=k)\n",
    "            return tuple(torch.round(random_matrix * vol).int().tolist())\n",
    "\n",
    "        print(\"INIT: Energy\", self.energy.mean() > self.target[0])\n",
    "        energy_flag = True\n",
    "        i = 0\n",
    "        while energy_flag:\n",
    "            self.energy = apply_hot_region(\n",
    "                self.energy,\n",
    "                rand_center(self.energy, mask_r),\n",
    "                tensor_vol(self.dim, self.vol * 0.003),\n",
    "                0,\n",
    "                3,\n",
    "                mode=0\n",
    "            )\n",
    "            i += 1\n",
    "            if (self.energy.mean() > self.target[0]) or (i == 50000):\n",
    "                energy_flag = False\n",
    "                print(\"if \", self.energy.mean() > self.target[0], \" or \", (i == 50000), \"flag: \", energy_flag)\n",
    "                print(\"self.energy.mean: \", self.energy.mean())\n",
    "\n",
    "        print(\"INIT: Wall\", self.wall.sum() > (self.target[1]))\n",
    "        wall_flag = True\n",
    "        i = 0\n",
    "        while wall_flag:\n",
    "            self.wall = apply_hot_region(\n",
    "                self.wall,\n",
    "                rand_center(self.wall, mask_r),\n",
    "                tensor_vol(self.dim, self.vol * 0.001),\n",
    "                1,\n",
    "                1,\n",
    "                mode=1\n",
    "            )\n",
    "            i += 1\n",
    "            if (self.wall.sum() > self.target[1]) or (i == 10000):\n",
    "                wall_flag = False\n",
    "                print(\"if \", self.wall.sum() > self.target[1], \" or \", (i == 10000), \"-> flag: \", wall_flag)\n",
    "                self.wall = set_tensor_edges_to_one(self.wall.clone())\n",
    "                self.wall = torch.bernoulli(self.wall)\n",
    "\n",
    "        print(\"INIT: Risk\", self.risk.mean() > (self.target[2]))\n",
    "        risk_flag = True\n",
    "        i = 0\n",
    "        if self.isRiskCluster:\n",
    "            while risk_flag:\n",
    "                self.risk = apply_hot_region(\n",
    "                    self.risk,\n",
    "                    rand_center(self.risk, mask_r),\n",
    "                    tensor_vol(self.dim, self.vol * 0.001),\n",
    "                    1,\n",
    "                    10,\n",
    "                    mode=0\n",
    "                )\n",
    "                i += 1\n",
    "                if (self.risk.mean() / 10 > self.target[2]) or (i == 10000):\n",
    "                    risk_flag = False\n",
    "                    print(\"if \", self.risk.mean() / 10 > self.target[2], \" or \", (i == 10000), \"flag: \", risk_flag)\n",
    "                    self.risk = torch.mul(self.risk, 0.1)\n",
    "\n",
    "        print(\"INIT: Death\", self.risk.mean() * self.target[3])\n",
    "        self.death = torch.bernoulli(torch.mul(self.risk, self.target[3]))\n",
    "\n",
    "    def env_global_features(self, len=3):\n",
    "        # Function to calculate feature sums for all axes\n",
    "        def compress_nd_matrix_sum(data) -> torch.tensor:\n",
    "            \"\"\"\n",
    "            Compress an n-dimensional matrix by summing along each axis using PyTorch.\n",
    "\n",
    "            Parameters:\n",
    "                data (torch.Tensor): An n-dimensional PyTorch tensor.\n",
    "            \"\"\"\n",
    "            n_dimensions = data.ndim\n",
    "            compressed_results = []\n",
    "            for axis in range(n_dimensions):\n",
    "                # Sum along the current axis and store the result\n",
    "                axis_sum = torch.sum(data, dim=axis)\n",
    "                compressed_results.append(axis_sum)\n",
    "            return torch.stack(compressed_results)\n",
    "\n",
    "        if len == 1:\n",
    "            return torch.stack([compress_nd_matrix_sum(self.map.clone())\n",
    "                                ])\n",
    "        elif len == 2:\n",
    "            return torch.stack([compress_nd_matrix_sum(self.map.clone()),\n",
    "                                compress_nd_matrix_sum(self.wall.clone())\n",
    "                                ])\n",
    "        elif len == 3:\n",
    "            return torch.stack([compress_nd_matrix_sum(self.energy.clone()),\n",
    "                                compress_nd_matrix_sum(self.risk.clone()),\n",
    "                                compress_nd_matrix_sum(self.wall.clone()),\n",
    "                                ])\n",
    "        elif len == 4:\n",
    "            return torch.stack([compress_nd_matrix_sum(self.energy.clone()),\n",
    "                                compress_nd_matrix_sum(self.risk.clone()),\n",
    "                                compress_nd_matrix_sum(self.wall.clone()),\n",
    "                                compress_nd_matrix_sum(self.death.clone())\n",
    "                                ])\n",
    "        elif len == 5:\n",
    "            return torch.stack([compress_nd_matrix_sum(self.energy.clone()),\n",
    "                                compress_nd_matrix_sum(self.risk.clone()),\n",
    "                                compress_nd_matrix_sum(self.wall.clone()),\n",
    "                                compress_nd_matrix_sum(self.death.clone()),\n",
    "                                compress_nd_matrix_sum(self.map.clone())\n",
    "                                ])\n",
    "\n",
    "    def env_local_state(self, loc, len=3):\n",
    "        def return_stack(loc, len=3):\n",
    "            if len == 1:\n",
    "                return torch.stack([pad_local_state(self.map.clone(), loc, len)\n",
    "                                    ])\n",
    "            elif len == 2:\n",
    "                return torch.stack([pad_local_state(self.map.clone(), loc, len),\n",
    "                                    pad_local_state(self.wall.clone(), loc, len)\n",
    "                                    ])\n",
    "            elif len == 3:\n",
    "                return torch.stack([pad_local_state(self.energy.clone(), loc, len),\n",
    "                                    pad_local_state(self.risk.clone(), loc, len),\n",
    "                                    pad_local_state(self.wall.clone(), loc, len)\n",
    "                                    ])\n",
    "            elif len == 4:\n",
    "                return torch.stack([pad_local_state(self.energy.clone(), loc, len),\n",
    "                                    pad_local_state(self.risk.clone(), loc, len),\n",
    "                                    pad_local_state(self.wall.clone(), loc, len),\n",
    "                                    pad_local_state(self.death.clone(), loc, len)\n",
    "                                    ])\n",
    "            elif len == 5:\n",
    "                return torch.stack([pad_local_state(self.energy.clone(), loc, len),\n",
    "                                    pad_local_state(self.risk.clone(), loc, len),\n",
    "                                    pad_local_state(self.wall.clone(), loc, len),\n",
    "                                    pad_local_state(self.death.clone(), loc, len),\n",
    "                                    pad_local_state(self.map.clone(), loc, len)\n",
    "                                    ])\n",
    "\n",
    "        def pad_local_state(input_tensor, location, distance):\n",
    "            # Create slice objects for each dimension\n",
    "            dim = input_tensor.dim()\n",
    "            slices = []\n",
    "            for i in range(dim):\n",
    "                start = location[i] - distance\n",
    "                end = location[i] + distance + 1  # +1 for exclusive end\n",
    "                slices.append(slice(max(0, start), min(input_tensor.size(i), end)))\n",
    "\n",
    "            # Extract the slice from the tensor\n",
    "            extracted = input_tensor[slices]\n",
    "\n",
    "            # Determine the shape of the full result with padding\n",
    "            full_shape = [2 * distance + 1] * dim\n",
    "\n",
    "            # Create a tensor of zeros with the target shape\n",
    "            result = torch.zeros(full_shape)\n",
    "\n",
    "            # Calculate the slices for inserting the extracted data into the result\n",
    "            insert_slices = [slice(max(0, distance - (location[i] - slices[i].start)),\n",
    "                                   max(0, distance - (location[i] - slices[i].start)) + extracted.size(i)) for i in\n",
    "                             range(dim)]\n",
    "\n",
    "            # Place the extracted slice into the padded result tensor\n",
    "            result[insert_slices] = extracted\n",
    "\n",
    "            return result\n",
    "\n",
    "        return return_stack(loc, len)\n",
    "\n",
    "    def roll(self, location):\n",
    "        if self.risk[location] > torch.rand(1):\n",
    "            return True, self.risk[location]\n",
    "        else:\n",
    "            return False, self.risk[location]\n",
    "\n",
    "    def get_cell_depreciated(self, location):\n",
    "        if not torch.is_tensor(location):\n",
    "            location = torch.tensor(location)\n",
    "\n",
    "        print(\"ENV.get_cell: \", location)\n",
    "        clamped_locations = torch.min(location, torch.tensor(self.map.shape) - 1)\n",
    "        return (self.energy[clamped_locations],\n",
    "                self.risk[clamped_locations],\n",
    "                self.wall[clamped_locations],\n",
    "                self.death[clamped_locations],\n",
    "                self.map[clamped_locations])\n",
    "\n",
    "    def get_cell(self, location):\n",
    "        def index_tensor(rand_tensor, locations):\n",
    "            locations = torch.as_tensor(locations)\n",
    "\n",
    "            # Ensure locations is a 2D tensor\n",
    "            if locations.dim() == 1:\n",
    "                locations = locations.unsqueeze(0)\n",
    "\n",
    "            # Clamp locations for each dimension\n",
    "            clamped_locations = torch.stack([\n",
    "                torch.clamp(locations[:, dim], 0, size - 1)\n",
    "                for dim, size in enumerate(rand_tensor.shape[:locations.size(1)])\n",
    "            ], dim=1)\n",
    "\n",
    "            # Use advanced indexing to select elements\n",
    "            return rand_tensor[tuple(clamped_locations.t())]\n",
    "\n",
    "        # if not torch.is_tensor(location):\n",
    "        #    location = torch.tensor(location, dtype=torch.long)\n",
    "        location = torch.as_tensor(location)\n",
    "        return (index_tensor(self.energy, location),\n",
    "                index_tensor(self.risk, location),\n",
    "                index_tensor(self.wall, location),\n",
    "                index_tensor(self.death, location),\n",
    "                index_tensor(self.map, location))\n",
    "\n",
    "\n",
    "class AgentWrapper:\n",
    "    def __init__(self, agent, info):\n",
    "        self.agent: AbstractRLAgent = agent\n",
    "        self.team = info['team']\n",
    "        self.end = info['end']\n",
    "        self.rewards = []\n",
    "        self.location = []\n",
    "        self.actions = []\n",
    "\n",
    "        self.location.append(info['start'])\n",
    "        self.rewards.append(0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        return self.agent.choose_action(state)\n",
    "\n",
    "    def update_model(self, state, action, reward, next_state, done):\n",
    "        self.agent.update_model(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "class AbstractRLAgent(ABC):\n",
    "    def __init__(self, action_space, state_space, info):\n",
    "        self.current_pos = info['current_pos']\n",
    "        self.end_pos = info['end_pos']\n",
    "        self.team = info['team']\n",
    "\n",
    "        self.action_size = action_space\n",
    "        self.state_size = state_space\n",
    "\n",
    "        self.cumulative_reward = 0\n",
    "        self.history = None\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Get the action to take based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state: The current state in the environment.\n",
    "\n",
    "        Returns:\n",
    "            An integer representing the chosen action.\n",
    "        \"\"\"\n",
    "        return self.policy.decide(state)\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_model(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the model based on the transition.\n",
    "\n",
    "        Args:\n",
    "            state: The current state from which the action was taken.\n",
    "            action: The action taken.\n",
    "            reward: The reward received after taking the action.\n",
    "            next_state: The state transitioned to after the action.\n",
    "            done: Boolean indicating if the episode has terminated.\n",
    "        \"\"\"\n",
    "        # Example learning procedure based on the reward and next state\n",
    "        pass\n",
    "\n",
    "\n",
    "class HelloAgent(AbstractRLAgent):\n",
    "    class LocalNN(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(HelloAgent.LocalNN, self).__init__()\n",
    "            self.name = \"HelloAgent\"\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, output_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "\n",
    "            self.filename = f\"models/NN-{self.name}-{self.input_dim}-{self.output_dim}.pt\"\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "        def save_model(self):\n",
    "            # Saving as TorchScript\n",
    "            model_scripted = torch.jit.script(self)\n",
    "            model_scripted.save(self.filename)\n",
    "            print(f\"Model saved {self.filename}\")\n",
    "\n",
    "        def load_model(self):\n",
    "            model_loaded = torch.jit.load(self.filename)\n",
    "            print(f\"Model loaded from {self.filename}\")\n",
    "            return model_loaded\n",
    "\n",
    "    #@autocast(device_type=torch.get_default_device())\n",
    "    def __init__(self, input_dim, output_dim, info):\n",
    "        self.output_size = output_dim\n",
    "        self.model = self.LocalNN(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=info['lr'])\n",
    "        self.criterion = info['criterion']()\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "        self.state_history = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(0, self.output_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state)\n",
    "                q_values = self.model(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "        print(f\"HelloAgent.choose_action(): {action}\")\n",
    "        return action\n",
    "\n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.model(next_state)\n",
    "        max_next_q_value = torch.max(next_q_values).item()\n",
    "\n",
    "        target_q_values = q_values.clone()\n",
    "        target_q_values[action] = reward + self.gamma * max_next_q_value  # Using Bellman Equation for target Q-value\n",
    "\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Record action, reward, and state\n",
    "        self.action_history.append(action)\n",
    "        self.reward_history.append(reward)\n",
    "        self.state_history.append(state.numpy())\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    # https://r-knott.surrey.ac.uk/Fibonacci/fibtable.html\n",
    "    def __init__(self, input_dim, output_dim, mode=0):\n",
    "        super(NN, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if self.mode == 0:\n",
    "            print(\"NN: \", self.mode)\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_dim, 610),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(610, 377),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(377, 144),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(144, 233),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(233, output_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=0.001)  # Assuming a learning rate is defined\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # Dummy behavior for random output, could be designed better based on specific requirements\n",
    "            self.layers = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 0:\n",
    "            return self.layers(x)\n",
    "        elif self.mode == 1:\n",
    "            return torch.rand((x.shape[0], self.output_dim))  # Random tensor with the same batch size as input\n",
    "\n",
    "    def save_model(self):\n",
    "        filename = f\"Zeo-NN-{self.mode}-{self.input_dim}-{self.output_dim}.pt\"\n",
    "        # Saving as TorchScript\n",
    "        model_scripted = torch.jit.script(self)\n",
    "        model_scripted.save(filename)\n",
    "        print(f\"Model saved as {filename}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        model_loaded = torch.jit.load(filepath)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model_loaded\n",
    "\n",
    "\n",
    "class Observer:\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "\n",
    "        self.agent_step_limit = self.env.vol * self.env.vol\n",
    "        self.MARL = True\n",
    "        self.LoadMap = False\n",
    "        self.LoadNavModel = False\n",
    "        self.LoadPvPModel = False\n",
    "        self.ResumeNav = False\n",
    "        self.ResumePvP = False\n",
    "\n",
    "        self.agent_store = []\n",
    "        self.log = []\n",
    "        self.Agents: List[AbstractRLAgent] = []\n",
    "        self.hard_L = self.env.map.max()\n",
    "\n",
    "        self.transitions = {\n",
    "            0: torch.tensor([2, 0, 0]),\n",
    "            1: torch.tensor([-2, 0, 0]),\n",
    "            2: torch.tensor([0, 2, 0]),\n",
    "            3: torch.tensor([0, -2, 0]),\n",
    "            4: torch.tensor([0, 0, 2]),\n",
    "            5: torch.tensor([0, 0, -2]),\n",
    "            6: torch.tensor([1, 1, 0]),\n",
    "            7: torch.tensor([1, -1, 0]),\n",
    "            8: torch.tensor([1, 0, 1]),\n",
    "            9: torch.tensor([1, 0, -1]),\n",
    "            10: torch.tensor([-1, 1, 0]),\n",
    "            11: torch.tensor([-1, -1, 0]),\n",
    "            12: torch.tensor([-1, 0, 1]),\n",
    "            13: torch.tensor([-1, 0, -1]),\n",
    "            14: torch.tensor([0, 0, 0])\n",
    "            # 15: None # TeleportAgent(exits, distance) - handled by a pointer network, or arm -> prevent slip\n",
    "        }\n",
    "        self.neighbours = torch.tensor([\n",
    "            [2, 0, 0],\n",
    "            [-2, 0, 0],\n",
    "            [0, 2, 0],\n",
    "            [0, -2, 0],\n",
    "            [0, 0, 2],\n",
    "            [0, 0, -2],\n",
    "            [1, 1, 0],\n",
    "            [1, -1, 0],\n",
    "            [1, 0, 1],\n",
    "            [1, 0, -1],\n",
    "            [-1, 1, 0],\n",
    "            [-1, -1, 0],\n",
    "            [-1, 0, 1],\n",
    "            [-1, 0, -1],\n",
    "            [0, 1, 1],\n",
    "            [0, -1, -1],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "        self.spawn_agents()\n",
    "        self.current_position = torch.tensor([5, 5, 5], dtype=torch.int64)\n",
    "\n",
    "    def train(self):\n",
    "        # Initialization of a training session\n",
    "        training_active = True\n",
    "        iteration = 0\n",
    "\n",
    "        while training_active:\n",
    "            iteration += 1\n",
    "            for index, agent_wrapper in enumerate(self.agent_store):\n",
    "                # Generate the current state for the agent based on its last observed location\n",
    "                state = self.observation(\n",
    "                    agent_wrapper.location[-1],\n",
    "                    agent_wrapper.end,\n",
    "                    agent_wrapper.team\n",
    "                )\n",
    "\n",
    "                # Agent chooses an action based on the current state\n",
    "                action = agent_wrapper.agent.choose_action(state)\n",
    "\n",
    "                # Environment processes the action and returns the reward and next observation\n",
    "                reward, next_observation = self.step(agent_wrapper, action)\n",
    "                agent_wrapper.actions.append(action)\n",
    "                agent_wrapper.rewards.append(reward)\n",
    "\n",
    "                # Update the agent model based on the action's outcomes\n",
    "                agent_wrapper.agent.update_model(state, action, reward, next_observation)\n",
    "\n",
    "                # Output the current step's results\n",
    "                print(f\"OBS.train(): loc | aim -> {agent_wrapper.location[-1]}, {agent_wrapper.end}\")\n",
    "                if agent_wrapper.location[-1] == agent_wrapper.end:\n",
    "                    print(\"OBS.train(): Agent has reached the target!\")\n",
    "                    training_active = False\n",
    "                    break\n",
    "\n",
    "            # Output training progress every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Observer.train(): Iteration {iteration}\")\n",
    "\n",
    "            # Terminate training after 1000 iterations\n",
    "            if iteration == 1000:\n",
    "                print(\"Observer.train(): Training complete!\")\n",
    "                training_active = False\n",
    "\n",
    "    def train_old_0(self):\n",
    "        # trains agents\n",
    "        # assign Policy, Agent pairs\n",
    "        i = 0\n",
    "        while self.train:\n",
    "            i += 1\n",
    "            for index in range(len(self.agent_store)):\n",
    "                state = self.observation(self.agent_store[index].location[-1],\n",
    "                                         self.agent_store[index].end,\n",
    "                                         self.agent_store[index].team)\n",
    "\n",
    "                action = self.agent_store[index].agent.choose_action(state)\n",
    "\n",
    "                reward, next_observation = self.step(self.agent_store[index], action)  # Presumed env interaction method\n",
    "                self.agent_store[index].action.append(action)\n",
    "                self.agent_store[index].rewards.append(reward)\n",
    "                self.agent_store[index].agent.update_model(state, action, reward, next_observation)\n",
    "                # collector for steps\n",
    "                print(\"OBS.train(): loc | aim -> \", self.agent_store[index].location[-1], self.agent_store[index].end)\n",
    "                if self.agent_store[index].location[-1] == self.agent_store[index].end:\n",
    "                    print(\"OBS.train(): Agent has reached the target!\")\n",
    "                    self.train = False\n",
    "                    break\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                # collector ouput\n",
    "                print(\"Observer.train(): \", i)\n",
    "                if i == 1000:\n",
    "                    print(\"Observer.train(): Training complete!\")\n",
    "                    self.train = False\n",
    "\n",
    "    def step(self, agent, action):\n",
    "        # Update the agent's position\n",
    "\n",
    "        next_position = self.update_agent_pos(action)\n",
    "        # Obtain the properties of the cell the agent is in\n",
    "        energy, risk, wall, death, map = self.env.get_cell(next_position)\n",
    "        # Calculate the reward based on the properties of the cell\n",
    "        reward = map + self.l('soft', next_position, intensity=0.99)\n",
    "        # Return the reward and the next observation\n",
    "        print(\"OBS.step.reward: \", reward)\n",
    "        next_observation = self.observation(next_position, agent.end, agent.team)\n",
    "        return reward, next_observation\n",
    "\n",
    "    def update_agent_pos(self, action):\n",
    "        # Update the current position\n",
    "        self.current_position += self.transitions[action]\n",
    "        print(\"OBS.uap.currentpos\", self.current_position)\n",
    "        # Ensure the position does not go out of bounds\n",
    "        return self.current_position\n",
    "\n",
    "    def CLF(self):\n",
    "        # should assess next worst possible state.\n",
    "        # Theorm -> agent in a ok state should not die because of a nudge\n",
    "        # like balancing a bottle, bottle at stable state ss should not fall down when k force is applied leading it to state sa or sb,\n",
    "        # however sc may be dangerous and sf is prohibited.\n",
    "        return\n",
    "\n",
    "    def l(self, mode, locations, intensity=0.99):\n",
    "        q = torch.tensor([0.38, 0.5, 0.62])\n",
    "        energy, risk, wall, death, map = self.env.get_cell(locations + self.neighbours)\n",
    "        print(\"OBS.l: \", energy.shape, risk.shape, wall.shape, death.shape, map.shape)\n",
    "        if mode == 'soft':  # reward shaping and considers any next state\n",
    "            # energy converges to 0, as min risk = 0\n",
    "            # you are now in a \"high\" region that is not perfered\n",
    "            score = (1 + risk.max()) * energy\n",
    "            score = torch.quantile(score.flatten().float(), q.float())\n",
    "            print(\"OBS.l.soft.tq.score: \", score)\n",
    "            score = torch.sum(score)\n",
    "            print(\"OBS.l.soft.score: \", score)\n",
    "            return score\n",
    "\n",
    "        elif mode == 'hard':  # per movement evaluation\n",
    "            # Checks next state if death as a hard limit.\n",
    "            # Does not allow agent to move to any cell adjacent to death\n",
    "            print(\"OBS.l.hard.bool: d, H*L ||\", death.max(), self.hard_L * intensity)\n",
    "            if death.max():\n",
    "                return True\n",
    "\n",
    "            if self.hard_L * intensity >= map.max():\n",
    "                return True\n",
    "\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        # ensure the trajectory converges to stable state,\n",
    "        # eg ground state -> energy equlibruim -> some minimizeable state\n",
    "        # policy: death=high, risk=high, wall=high\n",
    "        # can be returned as something to be minimised in the obj\n",
    "        # or operated as a constraint to clamp next possible states\n",
    "        # or be used to evaluate all next possible states\n",
    "\n",
    "        # intensity defines max acceptable state by evaluating worse case\n",
    "        # can be clamped by the mean or upper quantile l of l2 path.\n",
    "        # mode decides soft, hard, or expect\n",
    "        # soft is used for reward shaping, hard is used for constraint, expect is used for evaluation i states\n",
    "        # possible to dampen next state based on previous i states\n",
    "        # by exposing self to less risk energy state\n",
    "        # target can be lower than threshold or dynamic\n",
    "\n",
    "    def plot_space(self, mode):\n",
    "        if mode == 0:\n",
    "            print(\"env.energy: \", self.env.energy.min(), self.env.energy.mean(), self.env.energy.max())\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.energy, level=[0.1, 0.5, 0, 1, 3], opacity=0.03, level_width=0.1,\n",
    "                        data_min=self.env.energy.min(), data_max=self.env.energy.max())\n",
    "            ipv.show()\n",
    "\n",
    "        if mode == 1:\n",
    "            print(\"env.wall: \", self.env.wall.sum())\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.wall, level=[0, 1], opacity=0.05, level_width=0.1, data_min=self.env.wall.min(),\n",
    "                        data_max=self.env.wall.max())\n",
    "            ipv.show()\n",
    "\n",
    "        if mode == 2:\n",
    "            print(\"env.risk: \", self.env.risk.min(), self.env.risk.mean(), self.env.risk.max())\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.risk, level=[0, 0.25, 0.5, 0.75, 1], opacity=0.05, level_width=0.1,\n",
    "                        data_min=self.env.risk.min(), data_max=self.env.risk.max())\n",
    "            ipv.show()\n",
    "\n",
    "        if mode == 3:\n",
    "            print(\"env.death: \", self.env.death.min(), self.env.death.mean(), self.env.death.max())\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.death, opacity=0.05, level_width=0.1, data_min=self.env.death.min(),\n",
    "                        data_max=self.env.death.max())\n",
    "            ipv.show()\n",
    "\n",
    "        if mode == 4:\n",
    "            print(\"env.map: \", self.env.map.min(), self.env.map.mean(), self.env.map.max())\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.map, opacity=0.05, level_width=0.1, data_min=self.env.map.min(),\n",
    "                        data_max=self.env.map.max())\n",
    "            ipv.show()\n",
    "\n",
    "    def plot_slice(self, mode, loc=[14, 14, 7], r=7):\n",
    "        if mode == 0:\n",
    "            print(\"env.energy.slice: \")\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.energy[loc[0] - r:loc[0] + r, loc[1] - r:loc[1] + r, loc[2] - r:loc[2] + r],\n",
    "                        level=[0.1, 0.5, 0, 1, 3], opacity=0.03, level_width=0.1, data_min=0, data_max=3)\n",
    "            ipv.show()\n",
    "\n",
    "        if mode == 1:\n",
    "            print(\"env.wall.slice: \")\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.wall[loc[0] - r:loc[0] + r, loc[1] - r:loc[1] + r, loc[2] - r:loc[2] + r],\n",
    "                        level=[0, 1], opacity=0.05, level_width=0.1, data_min=0, data_max=1)\n",
    "            ipv.show()\n",
    "\n",
    "        if mode == 2:\n",
    "            print(\"env.risk.slice: \")\n",
    "            ipv.figure()\n",
    "            ipv.volshow(self.env.risk[loc[0] - r:loc[0] + r, loc[1] - r:loc[1] + r, loc[2] - r:loc[2] + r],\n",
    "                        level=[0, 0.25, 0.5, 0.75, 1], opacity=0.05, level_width=0.1, data_min=0, data_max=1)\n",
    "            ipv.show()\n",
    "\n",
    "    def plot_features(self, len=3):\n",
    "        # Obtain data from your environment function\n",
    "        tensors = self.env.env_global_features(len)\n",
    "        print(\"OBS.plot_features.shape is: \", tensors.shape)\n",
    "        # Determine the number of properties and scenarios dynamically\n",
    "        num_properties, num_scenarios, _, _ = tensors.shape  # Assume shape is [num_properties, num_scenarios, 10, 10]\n",
    "\n",
    "        # Prepare property names dynamically (if they are not predefined)\n",
    "        property_names = [f'Property {i + 1}' for i in range(num_properties)]\n",
    "\n",
    "        # Set up the figure based on the number of properties and scenarios\n",
    "        fig, axs = plt.subplots(num_properties, num_scenarios, figsize=(num_scenarios * 6, num_properties * 6))\n",
    "\n",
    "        # Check if we have a single row or column to adjust indexing\n",
    "        if num_properties == 1 or num_scenarios == 1:\n",
    "            axs = axs.reshape(num_properties, num_scenarios)\n",
    "\n",
    "        # Loop through each property and scenario\n",
    "        for i in range(num_properties):\n",
    "            for j in range(num_scenarios):\n",
    "                ax = axs[i, j]\n",
    "                sns.heatmap(tensors[i, j], cmap='hot', ax=ax, cbar_kws={'label': f'Level for {property_names[i]}'})\n",
    "                ax.set_title(f'{property_names[i]} - Scenario {j + 1}')\n",
    "\n",
    "        plt.tight_layout()  # Adjust subplots to fit into figure area.\n",
    "        plt.show()  # Show the plot\n",
    "\n",
    "    def observation(self, location, target, team, obs=3, k=3):\n",
    "        # Method to build obs for NAVagent\n",
    "        print(\"OBS: L, T = \", location, target)\n",
    "\n",
    "        def obs_map(loc, obs, k):\n",
    "            # 21 * 21 obs 441 pooled by k=3 to 49\n",
    "            mean_egf = F.avg_pool2d(self.env.env_global_features(len=obs).float(), kernel_size=k, stride=k)\n",
    "            # map: map,wall: energy,risk,wall: || 1->147, 2->294, 3->441\n",
    "            # energy,risk,wall,death: energy,risk,wall,death,map || 4->588, 5->735\n",
    "            # build local exact highres observation with vision 5\n",
    "            tensor_map = self.env.env_local_state(location, len=obs)\n",
    "            # returns the global foggy look and the local true observation\n",
    "            return torch.cat([mean_egf.flatten(), tensor_map.flatten()])\n",
    "\n",
    "        def obs_nav(location, target):\n",
    "            if not torch.is_tensor(location):\n",
    "                location = torch.tensor(location)\n",
    "\n",
    "                # Check if 'target' is a tensor, convert if not\n",
    "            if not torch.is_tensor(target):\n",
    "                target = torch.tensor(target)\n",
    "\n",
    "            # builds the relative properties for the NN, eg abs distance from target\n",
    "            return torch.cat([torch.tensor(location == target).flatten(),\n",
    "                              torch.tensor(target - location).flatten()])\n",
    "\n",
    "        return torch.cat([obs_map(location, obs, k), obs_nav(torch.tensor(location), target)])\n",
    "\n",
    "    def spawn_agents(self):\n",
    "        state_space = self.observation(location=torch.tensor([2, 2, 2]),\n",
    "                                       target=torch.tensor([19, 19, 19]),\n",
    "                                       team=0).shape[0]\n",
    "\n",
    "        cfg_helloagent = {\n",
    "            'lr': 1e-3,\n",
    "            'criterion': nn.MSELoss,\n",
    "            'epsilon': 0.1\n",
    "        }\n",
    "        self.Agents.append(HelloAgent(input_dim=state_space,\n",
    "                                      output_dim=len(self.transitions),\n",
    "                                      info=cfg_helloagent\n",
    "                                      ))\n",
    "        self.agent_store.append(AgentWrapper(self.Agents[-1], {'team': 0, 'start': [2, 2, 2], 'end': [19, 19, 19]}))\n",
    "        print(\"Observer.spawn_agents(): Agent | \", self.Agents)\n",
    "        print(\"Observer.spawn_agents(): AgentStore | \", self.agent_store)\n",
    "\n",
    "    def review(self):\n",
    "        # load torchscripts for agents\n",
    "\n",
    "        # snapshot environment\n",
    "        return\n",
    "\n",
    "    def review_PA(self):\n",
    "        # check policy agent pairs and compare with different PA pairs\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obs = Observer()\n",
    "    # obs.plot_space(0)\n",
    "    # obs.plot_space(1)\n",
    "    # obs.plot_space(2)\n",
    "    # obs.plot_space(3)\n",
    "    # obs.plot_space(4)\n",
    "    # obs.plot_features(5)\n",
    "    # obs.env.get_cell([5, 5, 5])\n",
    "\n",
    "    obs.train()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9261\n",
      "Setting Diff:  3  ->  (2, 2778.2999999999997, 0.3, 0.1)\n",
      "INIT: Volume 9261\n",
      "INIT: target:  (2, 2778.2999999999997, 0.3, 0.1)\n",
      "INIT: Energy tensor(False)\n",
      "if  tensor(False)  or  True flag:  False\n",
      "self.energy.mean:  tensor(0.8324)\n",
      "INIT: Wall tensor(False)\n",
      "if  tensor(True)  or  False -> flag:  False\n",
      "INIT: Risk tensor(False)\n",
      "if  tensor(True)  or  False flag:  False\n",
      "INIT: Death tensor(0.0301)\n",
      "OBS: L, T =  tensor([2, 2, 2]) tensor([19, 19, 19])\n",
      "Observer.spawn_agents(): Agent |  [<__main__.HelloAgent object at 0x32f30ef50>]\n",
      "Observer.spawn_agents(): AgentStore |  [<__main__.AgentWrapper object at 0x32edfa790>]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 13\n",
      "OBS.uap.currentpos tensor([4, 5, 4])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([4, 5, 4]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 1\n",
      "OBS.uap.currentpos tensor([2, 5, 4])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([2, 5, 4]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 1\n",
      "OBS.uap.currentpos tensor([0, 5, 4])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([0, 5, 4]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 4\n",
      "OBS.uap.currentpos tensor([0, 5, 6])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([0, 5, 6]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 4\n",
      "OBS.uap.currentpos tensor([0, 5, 8])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([0, 5, 8]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 1\n",
      "OBS.uap.currentpos tensor([-2,  5,  8])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([-2,  5,  8]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 10\n",
      "OBS.uap.currentpos tensor([-3,  6,  8])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([-3,  6,  8]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 10\n",
      "OBS.uap.currentpos tensor([-4,  7,  8])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([-4,  7,  8]) [19, 19, 19]\n",
      "OBS.train(): loc | aim -> [2, 2, 2], [19, 19, 19]\n",
      "OBS: L, T =  [2, 2, 2] [19, 19, 19]\n",
      "HelloAgent.choose_action(): 10\n",
      "OBS.uap.currentpos tensor([-5,  8,  8])\n",
      "OBS.l:  torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17]) torch.Size([17])\n",
      "OBS.l.soft.tq.score:  tensor([0., 0., 0.])\n",
      "OBS.l.soft.score:  tensor(0.)\n",
      "OBS.step.reward:  tensor([0.])\n",
      "OBS: L, T =  tensor([-5,  8,  8]) [19, 19, 19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_43080/3980894010.py:869: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.cat([obs_map(location, obs, k), obs_nav(torch.tensor(location), target)])\n",
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_43080/3980894010.py:866: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.cat([torch.tensor(location == target).flatten(),\n",
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_43080/3980894010.py:867: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(target - location).flatten()])\n",
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_43080/3980894010.py:484: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n",
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_43080/3980894010.py:492: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n",
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_43080/3980894010.py:493: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(next_state)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (0) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [0, 7, 7].  Tensor sizes: [20, 7, 7]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 910\u001B[0m\n\u001B[1;32m    901\u001B[0m obs \u001B[38;5;241m=\u001B[39m Observer()\n\u001B[1;32m    902\u001B[0m \u001B[38;5;66;03m# obs.plot_space(0)\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;66;03m# obs.plot_space(1)\u001B[39;00m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;66;03m# obs.plot_space(2)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    907\u001B[0m \u001B[38;5;66;03m# obs.plot_features(5)\u001B[39;00m\n\u001B[1;32m    908\u001B[0m \u001B[38;5;66;03m# obs.env.get_cell([5, 5, 5])\u001B[39;00m\n\u001B[0;32m--> 910\u001B[0m obs\u001B[38;5;241m.\u001B[39mtrain()\n",
      "Cell \u001B[0;32mIn[1], line 635\u001B[0m, in \u001B[0;36mObserver.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    632\u001B[0m action \u001B[38;5;241m=\u001B[39m agent_wrapper\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mchoose_action(state)\n\u001B[1;32m    634\u001B[0m \u001B[38;5;66;03m# Environment processes the action and returns the reward and next observation\u001B[39;00m\n\u001B[0;32m--> 635\u001B[0m reward, next_observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(agent_wrapper, action)\n\u001B[1;32m    636\u001B[0m agent_wrapper\u001B[38;5;241m.\u001B[39mactions\u001B[38;5;241m.\u001B[39mappend(action)\n\u001B[1;32m    637\u001B[0m agent_wrapper\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mappend(reward)\n",
      "Cell \u001B[0;32mIn[1], line 699\u001B[0m, in \u001B[0;36mObserver.step\u001B[0;34m(self, agent, action)\u001B[0m\n\u001B[1;32m    697\u001B[0m \u001B[38;5;66;03m# Return the reward and the next observation\u001B[39;00m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOBS.step.reward: \u001B[39m\u001B[38;5;124m\"\u001B[39m, reward)\n\u001B[0;32m--> 699\u001B[0m next_observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(next_position, agent\u001B[38;5;241m.\u001B[39mend, agent\u001B[38;5;241m.\u001B[39mteam)\n\u001B[1;32m    700\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m reward, next_observation\n",
      "Cell \u001B[0;32mIn[1], line 869\u001B[0m, in \u001B[0;36mObserver.observation\u001B[0;34m(self, location, target, team, obs, k)\u001B[0m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;66;03m# builds the relative properties for the NN, eg abs distance from target\u001B[39;00m\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat([torch\u001B[38;5;241m.\u001B[39mtensor(location \u001B[38;5;241m==\u001B[39m target)\u001B[38;5;241m.\u001B[39mflatten(),\n\u001B[1;32m    867\u001B[0m                       torch\u001B[38;5;241m.\u001B[39mtensor(target \u001B[38;5;241m-\u001B[39m location)\u001B[38;5;241m.\u001B[39mflatten()])\n\u001B[0;32m--> 869\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat([obs_map(location, obs, k), obs_nav(torch\u001B[38;5;241m.\u001B[39mtensor(location), target)])\n",
      "Cell \u001B[0;32mIn[1], line 853\u001B[0m, in \u001B[0;36mObserver.observation.<locals>.obs_map\u001B[0;34m(loc, obs, k)\u001B[0m\n\u001B[1;32m    849\u001B[0m mean_egf \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mavg_pool2d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39menv_global_features(\u001B[38;5;28mlen\u001B[39m\u001B[38;5;241m=\u001B[39mobs)\u001B[38;5;241m.\u001B[39mfloat(), kernel_size\u001B[38;5;241m=\u001B[39mk, stride\u001B[38;5;241m=\u001B[39mk)\n\u001B[1;32m    850\u001B[0m \u001B[38;5;66;03m# map: map,wall: energy,risk,wall: || 1->147, 2->294, 3->441\u001B[39;00m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;66;03m# energy,risk,wall,death: energy,risk,wall,death,map || 4->588, 5->735\u001B[39;00m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;66;03m# build local exact highres observation with vision 5\u001B[39;00m\n\u001B[0;32m--> 853\u001B[0m tensor_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39menv_local_state(location, \u001B[38;5;28mlen\u001B[39m\u001B[38;5;241m=\u001B[39mobs)\n\u001B[1;32m    854\u001B[0m \u001B[38;5;66;03m# returns the global foggy look and the local true observation\u001B[39;00m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat([mean_egf\u001B[38;5;241m.\u001B[39mflatten(), tensor_map\u001B[38;5;241m.\u001B[39mflatten()])\n",
      "Cell \u001B[0;32mIn[1], line 325\u001B[0m, in \u001B[0;36mEnvironment.env_local_state\u001B[0;34m(self, loc, len)\u001B[0m\n\u001B[1;32m    321\u001B[0m     result[insert_slices] \u001B[38;5;241m=\u001B[39m extracted\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m--> 325\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m return_stack(loc, \u001B[38;5;28mlen\u001B[39m)\n",
      "Cell \u001B[0;32mIn[1], line 279\u001B[0m, in \u001B[0;36mEnvironment.env_local_state.<locals>.return_stack\u001B[0;34m(loc, len)\u001B[0m\n\u001B[1;32m    275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack([pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m    276\u001B[0m                         pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwall\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m)\n\u001B[1;32m    277\u001B[0m                         ])\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack([pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menergy\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m    280\u001B[0m                         pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrisk\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m    281\u001B[0m                         pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwall\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m)\n\u001B[1;32m    282\u001B[0m                         ])\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack([pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menergy\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m    285\u001B[0m                         pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrisk\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m    286\u001B[0m                         pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwall\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m    287\u001B[0m                         pad_local_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeath\u001B[38;5;241m.\u001B[39mclone(), loc, \u001B[38;5;28mlen\u001B[39m)\n\u001B[1;32m    288\u001B[0m                         ])\n",
      "Cell \u001B[0;32mIn[1], line 321\u001B[0m, in \u001B[0;36mEnvironment.env_local_state.<locals>.pad_local_state\u001B[0;34m(input_tensor, location, distance)\u001B[0m\n\u001B[1;32m    316\u001B[0m insert_slices \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mslice\u001B[39m(\u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, distance \u001B[38;5;241m-\u001B[39m (location[i] \u001B[38;5;241m-\u001B[39m slices[i]\u001B[38;5;241m.\u001B[39mstart)),\n\u001B[1;32m    317\u001B[0m                        \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, distance \u001B[38;5;241m-\u001B[39m (location[i] \u001B[38;5;241m-\u001B[39m slices[i]\u001B[38;5;241m.\u001B[39mstart)) \u001B[38;5;241m+\u001B[39m extracted\u001B[38;5;241m.\u001B[39msize(i)) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m    318\u001B[0m                  \u001B[38;5;28mrange\u001B[39m(dim)]\n\u001B[1;32m    320\u001B[0m \u001B[38;5;66;03m# Place the extracted slice into the padded result tensor\u001B[39;00m\n\u001B[0;32m--> 321\u001B[0m result[insert_slices] \u001B[38;5;241m=\u001B[39m extracted\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The expanded size of the tensor (0) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [0, 7, 7].  Tensor sizes: [20, 7, 7]"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "795ad9c7-6cc7-4fdc-9cde-1520944f4acc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T20:45:37.513446Z",
     "start_time": "2024-05-13T20:45:37.509852Z"
    }
   },
   "source": [
    "    actual_slice = obs.observation(location=[14, 14, 7], target=[2, 18, 7], team=0, obs=3, k=3)\n",
    "    print(actual_slice.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS: L, T =  [14, 14, 7] [2, 18, 7]\n",
      "torch.Size([1476])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_6369/710406911.py:871: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.cat([torch.tensor(location == target).flatten(),\n",
      "/var/folders/lv/d9bgbh6s4vl8kfgyydwn949c0000gn/T/ipykernel_6369/710406911.py:872: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(target - location).flatten()])\n"
     ]
    }
   ],
   "execution_count": 246
  },
  {
   "cell_type": "code",
   "id": "6af6aab82b6fbc7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T20:37:04.208243Z",
     "start_time": "2024-05-13T20:37:04.205011Z"
    }
   },
   "source": [
    "#locations = [30, 5, 3]\n",
    "a, b, c, d, e = obs.env.get_cell(locations)\n",
    "#a\n",
    "#obs.env.energy[20]\n",
    "#rand_tensor=torch.rand([3,3,3])\n",
    "#rand_tensor"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9229, 0.5928, 0.3911],\n",
       "         [0.6313, 0.5713, 0.0806],\n",
       "         [0.9214, 0.4277, 0.3643]],\n",
       "\n",
       "        [[0.3701, 0.3672, 0.9951],\n",
       "         [0.7168, 0.4834, 0.7622],\n",
       "         [0.8691, 0.3931, 0.2642]],\n",
       "\n",
       "        [[0.9434, 0.9683, 0.2231],\n",
       "         [0.9854, 0.1147, 0.2100],\n",
       "         [0.7144, 0.2354, 0.9741]]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T20:37:34.117594Z",
     "start_time": "2024-05-13T20:37:34.114782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def index_tensor(rand_tensor, locations):\n",
    "    # Determine the number of dimensions in the tensor\n",
    "    num_dims = rand_tensor.dim()\n",
    "    \n",
    "    # Ensure the input locations are a 2D tensor [num_indices, num_dims]\n",
    "    if locations.dim() == 1:\n",
    "        locations = locations.unsqueeze(0)  # Convert to 2D if it's 1D\n",
    "    \n",
    "    # Clamp each dimension's indices to the valid range of that dimension\n",
    "    clamped_locations = torch.stack([\n",
    "        torch.clamp(locations[:, dim], 0, rand_tensor.shape[dim] - 1)\n",
    "        for dim in range(min(num_dims, locations.size(1)))\n",
    "    ], dim=1)\n",
    "    \n",
    "    # Extract elements based on clamped locations\n",
    "    indexed_elements = rand_tensor[clamped_locations[:, 0], clamped_locations[:, 1], clamped_locations[:, 2]]\n",
    "    return indexed_elements\n",
    "locations = torch.tensor([\n",
    "    [4, 4, 4],\n",
    "    [1, 2, 3],\n",
    "    [10, 10, 10]  # this will be clamped\n",
    "])\n",
    "indexed_elements = index_tensor(rand_tensor, locations)"
   ],
   "id": "3257b151732a439b",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T20:37:59.279499Z",
     "start_time": "2024-05-13T20:37:59.277210Z"
    }
   },
   "cell_type": "code",
   "source": "indexed_elements",
   "id": "bf9108660eada882",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9741, 0.2642, 0.9741])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 242
  },
  {
   "cell_type": "code",
   "id": "31a64d24-556e-4575-86e8-c74bd58372b8",
   "metadata": {},
   "source": [
    "import torch\n",
    "import ipyvolume as ipv\n",
    "\n",
    "def pad_local_state(input_tensor, distance, location):\n",
    "    # Create slice objects for each dimension\n",
    "    dim = input_tensor.dim()\n",
    "    slices = []\n",
    "    for i in range(dim):\n",
    "        start = location[i] - distance\n",
    "        end = location[i] + distance + 1  # +1 for exclusive end\n",
    "        slices.append(slice(max(0, start), min(input_tensor.size(i), end)))\n",
    "\n",
    "    # Extract the slice from the tensor\n",
    "    extracted = input_tensor[slices]\n",
    "\n",
    "    # Determine the shape of the full result with padding\n",
    "    full_shape = [2 * distance + 1] * dim\n",
    "\n",
    "    # Create a tensor of zeros with the target shape\n",
    "    result = torch.zeros(full_shape)\n",
    "\n",
    "    # Calculate the slices for inserting the extracted data into the result\n",
    "    insert_slices = [slice(max(0, distance - (location[i] - slices[i].start)),\n",
    "                           max(0, distance - (location[i] - slices[i].start)) + extracted.size(i)) for i in range(dim)]\n",
    "\n",
    "    # Place the extracted slice into the padded result tensor\n",
    "    result[insert_slices] = extracted\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_and_pad(input_tensor, distance, location):\n",
    "    # Ensure input_tensor is a PyTorch tensor and location is a tuple\n",
    "    input_tensor = torch.as_tensor(input_tensor)\n",
    "    location = tuple(location)\n",
    "\n",
    "    # Initialize slices for each dimension\n",
    "    slices = []\n",
    "    for i, loc in enumerate(location):\n",
    "        # Calculate start and end indices for each dimension\n",
    "        start = loc - distance\n",
    "        end = loc + distance + 1  # +1 for exclusive end\n",
    "\n",
    "        # Create a slice object for each dimension\n",
    "        slices.append(slice(max(0, start), min(input_tensor.size(i), end)))\n",
    "\n",
    "    # Extract the slice from the tensor\n",
    "    extracted = input_tensor[slices]\n",
    "\n",
    "    # Determine the shape of the full result with padding\n",
    "    full_shape = [2 * distance + 1] * input_tensor.dim()\n",
    "\n",
    "    # Create a tensor of zeros with the target shape\n",
    "    result = torch.zeros(full_shape, dtype=input_tensor.dtype, device=input_tensor.device)\n",
    "\n",
    "    # Calculate the slices for inserting the extracted data into the result\n",
    "    insert_slices = []\n",
    "    for i, slc in enumerate(slices):\n",
    "        start_idx = max(0, distance - location[i])\n",
    "        end_idx = start_idx + (slc.stop - slc.start)\n",
    "        insert_slices.append(slice(start_idx, end_idx))\n",
    "\n",
    "    # Place the extracted slice into the padded result tensor\n",
    "    result[insert_slices] = extracted\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Sample tensor and parameters\n",
    "sample_tensor = torch.ones(21, 21, 21)  # Example tensor\n",
    "\n",
    "# Test with different 'dimensions' parameter\n",
    "#padded_slice_2 = extract_and_pad(sample_tensor, [10, 10], 3, sample_tensor.dim(), 2)\n",
    "padded_slice_4 = extract_and_pad(sample_tensor, 3, [1, 20, 1])\n",
    "\n",
    "ps5 = pad_local_state(sample_tensor, 3, [1, 20, 1])\n",
    "\n",
    "print(ps5 == padded_slice_4)\n",
    "# Output shapes\n",
    "#print(\"Shape with last 2 dimensions processed:\", padded_slice_2.shape)\n",
    "print(\"Shape with last 4 dimensions processed:\", padded_slice_4.shape, padded_slice_4.min(), padded_slice_4.mean(), padded_slice_4.max())\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58620520-7cc8-4ff8-96f1-51e2585a4776",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4603cd11eb4feec",
   "metadata": {},
   "source": [
    "t = torch.full((8, 8), 0.5)\n",
    "torch.bernoulli(t).count_nonzero()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "629df74224328c53",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a sample tensor of shape [5, 3, 21, 21]\n",
    "x = torch.randn(5, 3, 21, 21)\n",
    "\n",
    "# Apply average pooling\n",
    "x_mean_pooled = F.avg_pool2d(x, kernel_size=3, stride=3)\n",
    "print(x_mean_pooled.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7f87f560392c734",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "c4e8cb8c-6a6d-4314-aa37-2b14ab801b78",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5065503-0b44-4935-a211-4440380d18dc",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef31791ba1ec45ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a46e32ad2ddd5cd3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8fb933a79da551e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10f92cae3461a18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea8bee7dce06e177",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6234e9684ef12d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def fill_hypercube(tensor, center, radius):\n",
    "    \"\"\"\n",
    "    Fill a hypercube area of a tensor with ones, centered at `center` with the specified `radius`.\n",
    "\n",
    "    Parameters:\n",
    "        tensor (torch.Tensor): An n-dimensional tensor.\n",
    "        center (tuple): A tuple representing the center of the hypercube.\n",
    "        radius (int): The radius of the hypercube to fill.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The modified tensor with ones in the specified hypercube area.\n",
    "    \"\"\"\n",
    "    # Generate slices for each dimension\n",
    "    slices = []\n",
    "    for i, c in enumerate(center):\n",
    "        start = max(c - radius, 0)\n",
    "        end = min(c + radius + 1, tensor.size(i))\n",
    "        slices.append(slice(start, end))\n",
    "\n",
    "    # Use tuple of slices to modify the tensor\n",
    "    tensor[tuple(slices)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Example usage:\n",
    "tensor = torch.zeros(5, 5, 5)  # A 3D tensor\n",
    "center = (2, 2, 2)  # Center of the hypercube\n",
    "radius = 1          # Radius of the hypercube\n",
    "\n",
    "modified_tensor = fill_hypercube(tensor, center, radius)\n",
    "print(modified_tensor)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8666bc1cf0ffd583",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Define the modified function for extracting local observations\n",
    "def extract_local_observation(properties, location, radius):\n",
    "    num_properties, *dimensions = properties.shape\n",
    "    assert len(location) == len(dimensions), \"Location must match the number of spatial dimensions in properties.\"\n",
    "\n",
    "    slices = []\n",
    "    padding = []\n",
    "    for dim, loc in zip(dimensions, location):\n",
    "        low = max(loc - radius, 0)\n",
    "        high = min(loc + radius + 1, dim)\n",
    "        slices.append(slice(low, high))\n",
    "        padding.append((max(radius - loc, 0), max(loc + radius + 1 - dim, 0)))\n",
    "\n",
    "    local_observation = properties[(slice(None),) + tuple(slices)]\n",
    "    padded_local_observation = torch.nn.functional.pad(local_observation, sum(padding[::-1], ()))\n",
    "\n",
    "    return padded_local_observation\n",
    "\n",
    "# Example 1: 3D Tensor\n",
    "properties_3d = torch.rand(5, 10, 10, 10)  # Shape: (num_properties, x_dim, y_dim, z_dim)\n",
    "location_3d = (5, 5, 5)\n",
    "radius_3d = 2\n",
    "extracted_3d = extract_local_observation(properties_3d, location_3d, radius_3d)\n",
    "print(\"Extracted Shape (3D):\", extracted_3d.shape)\n",
    "\n",
    "# Example 2: 4D Tensor\n",
    "properties_4d = torch.rand(3, 8, 8, 8, 8)  # Shape: (num_properties, x_dim, y_dim, z_dim, t_dim)\n",
    "location_4d = (4, 4, 4, 4)\n",
    "radius_4d = 1\n",
    "extracted_4d = extract_local_observation(properties_4d, location_4d, radius_4d)\n",
    "print(\"Extracted Shape (4D):\", extracted_4d.shape)\n",
    "\n",
    "# Example 3: 2D Tensor (edge case)\n",
    "properties_2d = torch.rand(6, 15, 15)  # Shape: (num_properties, x_dim, y_dim)\n",
    "location_2d = (7, 7)\n",
    "radius_2d = 3\n",
    "extracted_2d = extract_local_observation(properties_2d, location_2d, radius_2d)\n",
    "print(\"Extracted Shape (2D):\", extracted_2d.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bcddba2fdfb3d33",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0ccc28cdf58699b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "314121e1e6108441",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def set_tensor_edges_to_one(tensor):\n",
    "    \"\"\"\n",
    "    Sets the edges of an n-dimensional tensor to 1.\n",
    "\n",
    "    Args:\n",
    "    - tensor (torch.Tensor): An n-dimensional tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The modified tensor with its edges set to 1.\n",
    "    \"\"\"\n",
    "    # Iterate over each dimension and set the edge indices to 1\n",
    "    for dim in range(tensor.ndim):\n",
    "        # Get a list of all slice(None) initially which means select everything along each dimension\n",
    "        indexer = [slice(None)] * tensor.ndim\n",
    "\n",
    "        # Set the first and last index of the current dimension to 1\n",
    "        indexer[dim] = 0\n",
    "        tensor[tuple(indexer)] = 1\n",
    "        indexer[dim] = -1\n",
    "        tensor[tuple(indexer)] = 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "tensor_3d = torch.zeros(7, 7, 7)  # Creating a 3D tensor filled with zeros\n",
    "set_tensor_edges_to_one(tensor_3d)\n",
    "print(tensor_3d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33c4bf8bd54814f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43f06a0fec8fa70a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29ed9624a8918d6b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b907061c9d2b4e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35d6bcfbfef03d62",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54ffad345f46b8eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52d8e695f81e7241",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "706620f8f3727747",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6edcc49c31c4b198",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def set_random_dim_to_one(size):\n",
    "    return tuple(np.where(np.arange(len(size)) == np.random.randint(len(size)), 2, size))\n",
    "\n",
    "# Example usage\n",
    "size_tuple = (3, 5, 7)\n",
    "new_size = set_random_dim_to_one(size_tuple)\n",
    "print(\"Original size:\", size_tuple)\n",
    "print(\"Modified size:\", new_size)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5419da2b9cfad2f9",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def gaussian_tensor(size, std_dev):\n",
    "    \"\"\"\n",
    "    Create an n-dimensional Gaussian tensor in PyTorch using vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "    size (tuple of ints): the size of the tensor for each dimension.\n",
    "    std_dev (float): the standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: an n-dimensional Gaussian tensor.\n",
    "    \"\"\"\n",
    "    # Generate grid of indices for each dimension\n",
    "    ranges = [torch.linspace(0, s-1, steps=s) for s in size]\n",
    "    grid = torch.meshgrid(ranges, indexing='ij')\n",
    "    \n",
    "    # Calculate the center of the tensor\n",
    "    center = torch.tensor([(s-1)/2 for s in size])\n",
    "    \n",
    "    # Compute squared distance from the center\n",
    "    squared_distance = sum([(g - center[i])**2 for i, g in enumerate(grid)])\n",
    "    \n",
    "    # Apply the Gaussian formula\n",
    "    gaussian = torch.exp(-0.5 * (squared_distance / std_dev**2))\n",
    "    \n",
    "    # Normalize to max value of 1\n",
    "    gaussian = gaussian / torch.max(gaussian)\n",
    "    \n",
    "    return gaussian\n",
    "\n",
    "# Example usage:\n",
    "size = (3, 3, 3, 3)  # 3D tensor of size 10x10x10\n",
    "std_dev = 1.0        # Standard deviation\n",
    "tensor = gaussian_tensor(size, std_dev)\n",
    "print(torch.round(tensor*9))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0684b1089360b31",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "g = 9.81  # gravity\n",
    "l = 1.0   # length of pendulum\n",
    "m = 1.0   # mass of pendulum\n",
    "dt = 0.05 # time step\n",
    "k1, k2 = 10, 1  # Lyapunov function coefficients\n",
    "\n",
    "# Discretized Dynamics\n",
    "def update_state(theta, theta_dot, u):\n",
    "    theta_next = theta + dt * theta_dot\n",
    "    theta_dot_next = theta_dot + dt * (g / l * theta + u / (m * l**2))\n",
    "    return theta_next, theta_dot_next\n",
    "\n",
    "# Lyapunov Function\n",
    "def lyapunov(theta, theta_dot):\n",
    "    return 0.5 * k1 * theta**2 + 0.5 * k2 * theta_dot**2\n",
    "\n",
    "# Policy: Simple feedback to minimize Lyapunov function increase\n",
    "def control_policy(theta, theta_dot):\n",
    "    # Calculate necessary control to reduce Lyapunov function derivative\n",
    "    return -1.5 * (g / l + k1 / k2) * theta * theta_dot\n",
    "\n",
    "# Simulation\n",
    "def simulate_pendulum(steps=200):\n",
    "    theta, theta_dot = 0.2, 0  # small initial deviation from upright position\n",
    "    history = []\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        u = control_policy(theta, theta_dot)\n",
    "        theta, theta_dot = update_state(theta, theta_dot, u)\n",
    "        history.append((theta, theta_dot, u))\n",
    "        \n",
    "    return history\n",
    "\n",
    "# Run Simulation\n",
    "history = simulate_pendulum()\n",
    "\n",
    "# Plotting results\n",
    "thetas, theta_dots, us = zip(*history)\n",
    "t = np.arange(len(thetas)) * dt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(311)\n",
    "plt.plot(t, thetas, label='Theta')\n",
    "plt.ylabel('Angle (rad)')\n",
    "plt.title('Pendulum Angle')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(t, theta_dots, label='Theta Dot')\n",
    "plt.ylabel('Angular Velocity (rad/s)')\n",
    "plt.title('Pendulum Angular Velocity')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(t, us, label='Control Input')\n",
    "plt.ylabel('Torque (Nm)')\n",
    "plt.title('Control Input Applied')\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcffbc9a51726e20",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "def generate_binary_tensor(mean, shape):\n",
    "    # Create a tensor filled with the specified mean\n",
    "    probability_tensor = torch.full(shape, mean)\n",
    "    # Use Bernoulli distribution to generate binary tensor\n",
    "    binary_tensor = torch.bernoulli(probability_tensor)\n",
    "    return binary_tensor\n",
    "\n",
    "# Example usage\n",
    "mean = 0.9999  # Proportion of ones\n",
    "shape = (21, 21, 21)  # Shape of the tensor\n",
    "for i in range(5000):\n",
    "    tensor = generate_binary_tensor(mean, shape)\n",
    "    \n",
    "print(tensor.mean())\n",
    "tensor.min()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3028381861213fc1",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6d36c903a244ae1",
   "metadata": {},
   "source": [
    "from vmas import make_env\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from typing import Union\n",
    "import time\n",
    "import torch\n",
    "from vmas import make_env\n",
    "from vmas.simulator.core import Agent\n",
    "\n",
    "def _get_deterministic_action(agent: Agent, continuous: bool, env):\n",
    "    if continuous:\n",
    "        action = -agent.action.u_range_tensor.expand(env.batch_dim, agent.action_size)\n",
    "    else:\n",
    "        action = (\n",
    "            torch.tensor([1], device=env.device, dtype=torch.long)\n",
    "            .unsqueeze(-1)\n",
    "            .expand(env.batch_dim, 1)\n",
    "        )\n",
    "    return action.clone()\n",
    "\n",
    "def use_vmas_env(\n",
    "    render: bool,\n",
    "    num_envs: int,\n",
    "    n_steps: int,\n",
    "    device: str,\n",
    "    scenario: Union[str, BaseScenario],\n",
    "    continuous_actions: bool,\n",
    "    random_action: bool,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Example function to use a vmas environment.\n",
    "    \n",
    "    This is a simplification of the function in `vmas.examples.use_vmas_env.py`.\n",
    "\n",
    "    Args:\n",
    "        continuous_actions (bool): Whether the agents have continuous or discrete actions\n",
    "        scenario (str): Name of scenario\n",
    "        device (str): Torch device to use\n",
    "        render (bool): Whether to render the scenario\n",
    "        num_envs (int): Number of vectorized environments\n",
    "        n_steps (int): Number of steps before returning done\n",
    "        random_action (bool): Use random actions or have all agents perform the down action\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    scenario_name = scenario if isinstance(scenario,str) else scenario.__class__.__name__\n",
    "\n",
    "    env = make_env(\n",
    "        scenario=scenario,\n",
    "        num_envs=num_envs,\n",
    "        device=device,\n",
    "        continuous_actions=continuous_actions,\n",
    "        seed=0,\n",
    "        # Environment specific variables\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    frame_list = []  # For creating a gif\n",
    "    init_time = time.time()\n",
    "    step = 0\n",
    "\n",
    "    for s in range(n_steps):\n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}\")\n",
    "\n",
    "        actions = []\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            if not random_action:\n",
    "                action = _get_deterministic_action(agent, continuous_actions, env)\n",
    "            else:\n",
    "                action = env.get_random_action(agent)\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        obs, rews, dones, info = env.step(actions)\n",
    "\n",
    "        if render:\n",
    "            frame = env.render(\n",
    "                mode=\"rgb_array\",\n",
    "                agent_index_focus=None,  # Can give the camera an agent index to focus on\n",
    "            )\n",
    "            frame_list.append(frame)\n",
    "\n",
    "    total_time = time.time() - init_time\n",
    "    print(\n",
    "        f\"It took: {total_time}s for {n_steps} steps of {num_envs} parallel environments on device {device} \"\n",
    "        f\"for {scenario_name} scenario.\"\n",
    "    )\n",
    "\n",
    "    if render:\n",
    "        from moviepy.editor import ImageSequenceClip\n",
    "        fps=30\n",
    "        clip = ImageSequenceClip(frame_list, fps=fps)\n",
    "        clip.write_gif(f'{scenario_name}.gif', fps=fps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17803ea706a3d090",
   "metadata": {},
   "source": [
    "scenario_name=\"discovery\"\n",
    "use_vmas_env(\n",
    "    scenario=scenario_name,\n",
    "    render=True,\n",
    "    num_envs=4096,\n",
    "    n_steps=1000,\n",
    "    device=\"cpu\",\n",
    "    continuous_actions=False,\n",
    "    random_action=False,\n",
    "    # Environment specific variables\n",
    "    n_agents=4,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdd308881dd82cdd",
   "metadata": {},
   "source": [
    "def process_numbers(min=0, max=None):\n",
    "    if max is None:\n",
    "        # Handle the case where max is not provided\n",
    "        print(f\"Only min is specified: min={min}\")\n",
    "    else:\n",
    "        # Both min and max are provided\n",
    "        print(f\"Min and max are specified: min={min}, max={max}\")\n",
    "\n",
    "# Example usage\n",
    "process_numbers()          # Uses default min=0 and max=None\n",
    "process_numbers(10)        # Uses min=10 and default max=None\n",
    "process_numbers(10, 20)    # Uses min=10 and max=20\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f843641-fcb7-44f6-be69-55d822e0c679",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "\n",
    "# Threshold k\n",
    "k = 3\n",
    "\n",
    "# Zero out values below k\n",
    "tensor[tensor < k] = 0\n",
    "print(tensor)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2097b8a-b2e7-4bca-96fb-522f5028bf97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T17:13:30.090841Z",
     "start_time": "2024-05-12T17:13:30.084464Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a base class for policies\n",
    "class Policy(nn.Module):\n",
    "    def forward(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Define a specific policy, e.g., a simple neural network\n",
    "class SimplePolicy(Policy):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "# Define a base class for models (environment models or value functions)\n",
    "class Model(nn.Module):\n",
    "    def forward(self, *inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Define a simple model for predicting next state and reward\n",
    "class SimpleModel(Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, state_dim + 1)  # state_dim states and 1 reward\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        input = torch.cat([state, action], dim=-1)\n",
    "        return self.network(input)\n",
    "\n",
    "# Define a base class for algorithms\n",
    "class Algorithm:\n",
    "    def update_policy(self, policy, model):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Define a specific algorithm, e.g., Q-learning\n",
    "class QLearning(Algorithm):\n",
    "    def update_policy(self, policy, model):\n",
    "        # Placeholder for the actual Q-learning update logic\n",
    "        pass\n",
    "\n",
    "# Define the Agent class\n",
    "class Agent:\n",
    "    def __init__(self, policy, model, algorithm):\n",
    "        self.policy = policy\n",
    "        self.model = model\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.policy(state).argmax().item()\n",
    "\n",
    "    def learn(self):\n",
    "        self.algorithm.update_policy(self.policy, self.model)\n",
    "\n",
    "# Example of creating and using multiple agents\n",
    "def main():\n",
    "    num_agents = 3\n",
    "    agents = []\n",
    "    for i in range(num_agents):\n",
    "        state_dim = 10\n",
    "        action_dim = 5\n",
    "        policy = SimplePolicy(state_dim, action_dim)\n",
    "        model = SimpleModel(state_dim, action_dim)\n",
    "        algorithm = QLearning()\n",
    "        agent = Agent(policy, model, algorithm)\n",
    "        agents.append(agent)\n",
    "    \n",
    "    # Example of evaluating agents\n",
    "    state = torch.randn(10)  # Example state\n",
    "    for idx, agent in enumerate(agents):\n",
    "        action = agent.act(torch.unsqueeze(state, 0))\n",
    "        print(f\"Agent {idx} action: {action}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 action: 1\n",
      "Agent 1 action: 3\n",
      "Agent 2 action: 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:07:32.076373Z",
     "start_time": "2024-05-12T19:07:31.995090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_default_dtype(torch.float32)\n",
    "# Define the simple linear model\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One input and one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "# Inputs (x) from -10 to 10\n",
    "x = torch.arange(-10, 11, 1.0).unsqueeze(1)\n",
    "# Outputs (y) follow the function y = 2x + 1\n",
    "y_true = 2 * x + x * torch.exp(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLinearModel()\n",
    "\n",
    "# Mean Squared Error (MSE) loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Stochastic Gradient Descent (SGD) optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_true)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ],
   "id": "18eac5090826050a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1750925056.0000\n",
      "Epoch [200/1000], Loss: 1746531840.0000\n",
      "Epoch [300/1000], Loss: 1746454656.0000\n",
      "Epoch [400/1000], Loss: 1746453504.0000\n",
      "Epoch [500/1000], Loss: 1746453248.0000\n",
      "Epoch [600/1000], Loss: 1746453120.0000\n",
      "Epoch [700/1000], Loss: 1746453248.0000\n",
      "Epoch [800/1000], Loss: 1746453248.0000\n",
      "Epoch [900/1000], Loss: 1746453248.0000\n",
      "Epoch [1000/1000], Loss: 1746453248.0000\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T19:07:32.151690Z",
     "start_time": "2024-05-12T19:07:32.148735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "\n",
    "# Print out the learned parameters and a few predictions\n",
    "print(\"Learned parameters:\", list(model.parameters()))\n",
    "print(\"Some actual vs. predicted values:\")\n",
    "print(torch.cat([y_true[:5], y_pred[:5]], 1))\n"
   ],
   "id": "ce024ec8ea6f7f77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters: [Parameter containing:\n",
      "tensor([[4057.6301]], requires_grad=True), Parameter containing:\n",
      "tensor([15627.3281], requires_grad=True)]\n",
      "Some actual vs. predicted values:\n",
      "tensor([[-2.0000e+01, -2.4949e+04],\n",
      "        [-1.8001e+01, -2.0891e+04],\n",
      "        [-1.6003e+01, -1.6834e+04],\n",
      "        [-1.4006e+01, -1.2776e+04],\n",
      "        [-1.2015e+01, -8.7185e+03]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:23:50.932065Z",
     "start_time": "2024-05-13T13:23:38.424355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import autocast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "logdir = \"runs\"\n",
    "tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, mode=0):\n",
    "        super(NN, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model_path = None\n",
    "\n",
    "        if self.mode == 0:\n",
    "            self.model_path = f\"model/NN-mode-{self.mode}-{self.input_dim}-{self.output_dim}.pt\"\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_dim, 610),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(610, 377),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(377, 144),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(144, 233),\n",
    "                nn.SELU(),\n",
    "                nn.Linear(233, output_dim),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=0.001)  # Assuming a learning rate is defined\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # Dummy behavior for random output, could be designed better based on specific requirements\n",
    "            self.layers = None\n",
    "    #@autocast(device_type=torch.get_default_device())\n",
    "    def forward(self, x):\n",
    "        if self.mode == 0:\n",
    "            return self.layers(x)\n",
    "        elif self.mode == 1:\n",
    "            return torch.rand((x.shape[0], self.output_dim))  # Random tensor with the same batch size as input\n",
    "\n",
    "    def save_model(self):\n",
    "        filename = f\"NN-mode-{self.mode}-{self.input_dim}-{self.output_dim}.pt\"\n",
    "        # Saving as TorchScript\n",
    "        model_scripted = torch.jit.script(self)\n",
    "        model_scripted.save(filename)\n",
    "        print(f\"Model saved as {filename}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        model_loaded = torch.jit.load(self.model_path)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model_loaded\n",
    "    \n",
    "class TrainingEnvironment:\n",
    "    def __init__(self, model, learning_rate=1):\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.loss_history = []\n",
    "        self.writer = SummaryWriter()  # Initializing TensorBoard\n",
    "\n",
    "    def train(self, data_generator, epochs):\n",
    "        x, y_true = data_generator.get_data()\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y_true)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if epoch == 0:  # Log the model graph only once\n",
    "                self.writer.add_graph(self.model, x)\n",
    "                \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                self.writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    self.writer.add_histogram(name, param, epoch)\n",
    "                \n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    def evaluate(self, data_generator):\n",
    "        self.model.eval()\n",
    "        x, y_true = data_generator.get_data()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "        self.writer.add_graph(self.model, x)  # Log the model graph\n",
    "        return y_pred\n",
    "    \n",
    "class DataGenerator:\n",
    "    def __init__(self):\n",
    "        self.x = torch.arange(-20, 20, 0.5).unsqueeze(1)\n",
    "        self.y_true = self.x * self.x * self.x + 2 * self.x * self.x + 1  # y = 2x + 1\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.x, self.y_true\n",
    "\n",
    "def plot_results(model, data_generator, training_environment):\n",
    "    x, y_true = data_generator.get_data()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "\n",
    "    # Plotting actual vs predicted values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Subplot for the regression line\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x.numpy(), y_true.numpy(), color='blue', label='Actual')\n",
    "    plt.scatter(x.numpy(), y_pred.numpy(), color='red', marker='x', label='Predicted')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot for the loss history\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_environment.loss_history, label='Loss history')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Instantiate the classes\n",
    "policy = NN(input_dim=1, output_dim=1)\n",
    "data_generator = DataGenerator()\n",
    "training_environment = TrainingEnvironment(model=policy, learning_rate=0.003)\n",
    "\n",
    "# Train the model\n",
    "training_environment.train(data_generator, epochs=5000)\n",
    "\n",
    "# Evaluate the model\n",
    "policy.eval()\n",
    "plot_results(policy, data_generator, training_environment)\n",
    "with torch.no_grad():\n",
    "    x, y_true = data_generator.get_data()\n",
    "    y_pred = policy(x)\n",
    "    print(\"Some actual vs. predicted values:\")\n",
    "    print(torch.cat([y_true[:5], y_pred[:5]], 1))\n"
   ],
   "id": "78da021b31b9c208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: runs, tensor debug mode: FULL_HEALTH)\n",
      "Epoch [100/5000], Loss: 17120.7148\n",
      "Epoch [200/5000], Loss: 660.3607\n",
      "Epoch [300/5000], Loss: 1777.0183\n",
      "Epoch [400/5000], Loss: 25416.0977\n",
      "Epoch [500/5000], Loss: 7405.6460\n",
      "Epoch [600/5000], Loss: 14305.7598\n",
      "Epoch [700/5000], Loss: 2822.1492\n",
      "Epoch [800/5000], Loss: 327.2310\n",
      "Epoch [900/5000], Loss: 594.1395\n",
      "Epoch [1000/5000], Loss: 898.9380\n",
      "Epoch [1100/5000], Loss: 387.3743\n",
      "Epoch [1200/5000], Loss: 625.8927\n",
      "Epoch [1300/5000], Loss: 3169.2815\n",
      "Epoch [1400/5000], Loss: 179.8922\n",
      "Epoch [1500/5000], Loss: 1488.3849\n",
      "Epoch [1600/5000], Loss: 3004.4795\n",
      "Epoch [1700/5000], Loss: 5746.5933\n",
      "Epoch [1800/5000], Loss: 59.2839\n",
      "Epoch [1900/5000], Loss: 5904.6714\n",
      "Epoch [2000/5000], Loss: 8572.2393\n",
      "Epoch [2100/5000], Loss: 80.2423\n",
      "Epoch [2200/5000], Loss: 166.9937\n",
      "Epoch [2300/5000], Loss: 1304.9856\n",
      "Epoch [2400/5000], Loss: 243.0858\n",
      "Epoch [2500/5000], Loss: 7903.6357\n",
      "Epoch [2600/5000], Loss: 3206.5422\n",
      "Epoch [2700/5000], Loss: 55.1723\n",
      "Epoch [2800/5000], Loss: 2265.3940\n",
      "Epoch [2900/5000], Loss: 962.0708\n",
      "Epoch [3000/5000], Loss: 789.5586\n",
      "Epoch [3100/5000], Loss: 572.7559\n",
      "Epoch [3200/5000], Loss: 784.6262\n",
      "Epoch [3300/5000], Loss: 143.8322\n",
      "Epoch [3400/5000], Loss: 91.6809\n",
      "Epoch [3500/5000], Loss: 2114.3120\n",
      "Epoch [3600/5000], Loss: 1606.1678\n",
      "Epoch [3700/5000], Loss: 615.8473\n",
      "Epoch [3800/5000], Loss: 943.1629\n",
      "Epoch [3900/5000], Loss: 5610.5850\n",
      "Epoch [4000/5000], Loss: 10223.4238\n",
      "Epoch [4100/5000], Loss: 759.9964\n",
      "Epoch [4200/5000], Loss: 343.6608\n",
      "Epoch [4300/5000], Loss: 123.7096\n",
      "Epoch [4400/5000], Loss: 1767.8571\n",
      "Epoch [4500/5000], Loss: 699.0619\n",
      "Epoch [4600/5000], Loss: 1151.0837\n",
      "Epoch [4700/5000], Loss: 3176.3232\n",
      "Epoch [4800/5000], Loss: 2622.7778\n",
      "Epoch [4900/5000], Loss: 1469.7692\n",
      "Epoch [5000/5000], Loss: 96.7641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some actual vs. predicted values:\n",
      "tensor([[-7199.0000, -7224.3164],\n",
      "        [-6653.3750, -6692.4097],\n",
      "        [-6136.0000, -6169.3755],\n",
      "        [-5646.1250, -5666.7861],\n",
      "        [-5183.0000, -5202.0269]])\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:28:04.576099Z",
     "start_time": "2024-05-13T13:28:04.573308Z"
    }
   },
   "cell_type": "code",
   "source": "policy(torch.tensor([1], dtype=torch.float32))",
   "id": "a944297202beed10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2234], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:28:06.625420Z",
     "start_time": "2024-05-13T13:28:06.623240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = 1\n",
    "x * x * x + 2 * x * x + 1"
   ],
   "id": "3b6148bd0b2ab9ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T13:25:20.890207Z",
     "start_time": "2024-05-13T13:23:51.794150Z"
    }
   },
   "cell_type": "code",
   "source": "!tensorboard --logdir=runs",
   "id": "318c83c346b29459",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\n",
      "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\r\n",
      "^C\r\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T09:30:54.412899Z",
     "start_time": "2024-05-13T09:30:54.402284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class TensorUpdater:\n",
    "    def __init__(self):\n",
    "        # Define transitions\n",
    "        self.transitions = {\n",
    "            0: torch.tensor([2, 0, 0]),\n",
    "            1: torch.tensor([-2, 0, 0]),\n",
    "            2: torch.tensor([0, 2, 0]),\n",
    "            3: torch.tensor([0, -2, 0]),\n",
    "            4: torch.tensor([0, 0, 2]),\n",
    "            5: torch.tensor([0, 0, -2]),\n",
    "            6: torch.tensor([1, 1, 0]),\n",
    "            7: torch.tensor([1, -1, 0]),\n",
    "            8: torch.tensor([1, 0, 1]),\n",
    "            9: torch.tensor([1, 0, -1]),\n",
    "            10: torch.tensor([-1, 1, 0]),\n",
    "            11: torch.tensor([-1, -1, 0]),\n",
    "            12: torch.tensor([-1, 0, 1]),\n",
    "            13: torch.tensor([-1, 0, -1]),\n",
    "            14: torch.tensor([0, 0, 0])\n",
    "        }\n",
    "        # Initialize some example 3D tensor\n",
    "        self.data = torch.arange(1000).view(10, 10, 10)  # Example tensor filled with values\n",
    "        # Starting index as a tuple\n",
    "        self.current_position = torch.tensor([5, 5, 5], dtype=torch.int64)\n",
    "\n",
    "    def get_element_at_position(self, action):\n",
    "        # Get transition based on agent's action\n",
    "        transition = self.transitions[action]\n",
    "        # Update the current position\n",
    "        self.current_position += transition\n",
    "        print(self.current_position)\n",
    "        # Ensure the position does not go out of bounds\n",
    "        self.current_position = torch.clamp(self.current_position, 0, 9)\n",
    "        # Return the tensor element at the new position\n",
    "        element = self.env.get_cell(self.current_position)\n",
    "        return element\n",
    "\n",
    "# Example usage\n",
    "updater = TensorUpdater()\n",
    "action = 0  # Agent's response, for example\n",
    "element_at_new_position = updater.get_element_at_position(action)\n",
    "print(f\"Element at the new position: {element_at_new_position}\")\n"
   ],
   "id": "260d5675e10f9299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 5, 5])\n",
      "Element at the new position: 755\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T09:30:57.560715Z",
     "start_time": "2024-05-13T09:30:57.556763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "action = 13  # Agent's response, for example\n",
    "element_at_new_position = updater.get_element_at_position(action)\n",
    "print(f\"Element at the new position: {element_at_new_position}\")"
   ],
   "id": "febc51be2ad9cfbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 4])\n",
      "Element at the new position: 654\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T10:19:14.394151Z",
     "start_time": "2024-05-13T10:19:14.388882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, tensor_dimensions):\n",
    "        # Create a tensor with specified dimensions filled with random values\n",
    "        self.tensor = torch.rand(*tensor_dimensions)\n",
    "        # Define transitions; the dimensionality of the transitions should match the tensor\n",
    "        # Here we define a simple example which works only for 3-dimensional tensors,\n",
    "        # You should adjust or dynamically generate this based on tensor_dimensions.\n",
    "        self.transitions = torch.tensor([\n",
    "            [2, 0, 0],\n",
    "            [-2, 0, 0],\n",
    "            [0, 2, 0],\n",
    "            [0, -2, 0],\n",
    "            [0, 0, 2],\n",
    "            [0, 0, -2],\n",
    "            [1, 1, 0],\n",
    "            [1, -1, 0],\n",
    "            [1, 0, 1],\n",
    "            [1, 0, -1],\n",
    "            [-1, 1, 0],\n",
    "            [-1, -1, 0],\n",
    "            [-1, 0, 1],\n",
    "            [-1, 0, -1],\n",
    "            [0, 1, 1],\n",
    "            [0, -1, -1],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "\n",
    "    def area(self, location):\n",
    "        # Ensure location is a tensor and matches the tensor dimensions\n",
    "        location = torch.tensor(location, dtype=torch.int64)\n",
    "        # Add location to each transition to get a new set of locations\n",
    "        new_locations = self.transitions + location[None, :]\n",
    "        # Call _get_cell vectorized\n",
    "        return self._get_cell(new_locations)\n",
    "\n",
    "    def _get_cell(self, locations):\n",
    "        # Clamp each dimension of locations to be within the tensor dimensions\n",
    "        clamped_locations = torch.stack([\n",
    "            torch.clamp(locations[:, i], 0, self.tensor.size(i) - 1) for i in range(self.tensor.dim())\n",
    "        ], dim=1)\n",
    "        # Use advanced indexing to retrieve multiple values from self.tensor\n",
    "        return self.tensor[tuple(clamped_locations.t())]\n",
    "\n",
    "# Example usage for a 3-dimensional tensor\n",
    "mc = MyClass([10, 10, 10])  # Specify the dimensions of the tensor\n",
    "print(mc.area([0, 0, 0]))  # Example location in the middle of the tensor\n"
   ],
   "id": "5aa81856e5c996bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5360, 0.2077, 0.1030, 0.2077, 0.9793, 0.2077, 0.4118, 0.4640, 0.9930,\n",
      "        0.4640, 0.9501, 0.2077, 0.7424, 0.2077, 0.3062, 0.2077, 0.2077])\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:01:44.373823Z",
     "start_time": "2024-05-13T21:01:44.370955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = mc.area([0, 0, 0])\n",
    "t.dtype"
   ],
   "id": "656c0bb8bcd9d554",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 255
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T11:24:49.732136Z",
     "start_time": "2024-05-13T11:24:49.725446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q = torch.tensor([0.38, 0.5, 0.62])\n",
    "print(torch.quantile(t.flatten(), q))\n"
   ],
   "id": "b038d8b96b65effd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2077, 0.3062, 0.4598])\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T21:02:02.633892Z",
     "start_time": "2024-05-13T21:02:02.631505Z"
    }
   },
   "cell_type": "code",
   "source": "q.dtype",
   "id": "d0c18d9c3d245ce6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T11:25:06.899527Z",
     "start_time": "2024-05-13T11:25:06.896941Z"
    }
   },
   "cell_type": "code",
   "source": "t.max()",
   "id": "1f16864aa271ff3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9930)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T15:34:35.152382Z",
     "start_time": "2024-05-13T15:34:35.150495Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "aed9a45ee54876c2",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:25:57.168220Z",
     "start_time": "2024-05-13T16:25:57.165334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Book:\n",
    "    def __init__(self, title, author, pages):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.pages = pages\n",
    "        \n",
    "    def call(self):\n",
    "        print(f\"Hello Title: {self.title} by {self.author} with {self.pages} pages.\")\n",
    "\n",
    "library = [Book(\"Title1\", \"Author1\", 100)]\n",
    "library.append(Book(\"Title2\", \"Author2\", 200))\n",
    "library[0].call()\n",
    "len(library)"
   ],
   "id": "9ab982116c3f36ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Title: Title1 by Author1 with 100 pages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:38:57.620496Z",
     "start_time": "2024-05-13T16:38:57.617961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Book:\n",
    "    def __init__(self, title):\n",
    "        self.title = title\n",
    "    \n",
    "    def display_title(self):\n",
    "        print(f\"Title: {self.title}\")\n",
    "\n",
    "class BookWrapper:\n",
    "    def __init__(self, book):\n",
    "        self.book_class = book\n",
    "\n",
    "# Create instances of Book\n",
    "book1 = Book(\"1984\")\n",
    "book2 = Book(\"Animal Farm\")\n",
    "\n",
    "# Wrap the books\n",
    "wrapper1 = BookWrapper(book1)\n",
    "wrapper2 = BookWrapper(book2)\n",
    "\n",
    "# List of wrappers\n",
    "wrapper_list = [wrapper1, wrapper2]\n",
    "\n",
    "# Accessing and invoking a method on the second book\n",
    "wrapper_list[1].book_class.display_title()\n"
   ],
   "id": "777fc740b184f69e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Animal Farm\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T17:09:15.865707Z",
     "start_time": "2024-05-13T17:09:15.863569Z"
    }
   },
   "cell_type": "code",
   "source": "np.random.rand()",
   "id": "8f9ceebc597139cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9017859978317155"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T20:05:19.930816Z",
     "start_time": "2024-05-13T20:05:19.927133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Assuming tensor_a is already defined with shape [21, 21, 21]\n",
    "# Let's create an example tensor_a filled with random numbers for demonstration\n",
    "tensor_a = torch.randn(7, 7, 7)\n",
    "tensor_a"
   ],
   "id": "af1cd95c4a105538",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3086, -2.1367, -2.7246,  1.8213, -0.2939,  0.5400,  1.6807],\n",
       "         [-1.6426,  0.6841, -1.1709, -1.5977,  0.1907,  0.5703, -2.6406],\n",
       "         [ 0.6602, -1.5830, -1.0117,  0.5190, -0.3171,  1.8896,  0.1170],\n",
       "         [-0.6250, -0.0053, -0.0861,  1.1572,  0.2900, -1.0039,  0.0696],\n",
       "         [-1.6973, -1.3447,  1.5635,  1.4082,  1.0957,  1.6406, -0.5146],\n",
       "         [-1.4082,  0.3250,  0.8818, -0.4639, -1.2051, -1.2266, -0.8672],\n",
       "         [-0.3423, -0.8521, -0.5083, -0.7993,  0.5303,  0.0223, -2.4434]],\n",
       "\n",
       "        [[-0.6064, -1.5674, -0.4419,  2.1484, -0.5503, -1.1348,  1.6973],\n",
       "         [ 1.1748,  0.0481,  1.4922, -2.1309, -0.8799,  0.8813,  2.5371],\n",
       "         [ 1.0029,  0.1901,  0.5576, -1.9775, -1.6904, -0.9150, -1.1367],\n",
       "         [ 0.4387,  0.2761,  1.6631, -0.0712,  1.7197,  0.6440, -1.5117],\n",
       "         [ 0.1643, -1.2305,  0.2035, -0.3157,  1.5898,  0.9927, -0.6313],\n",
       "         [-0.0715,  0.6538,  0.4482,  0.5024,  0.1340,  0.4192, -0.6621],\n",
       "         [ 0.6704, -0.2654, -0.3689, -0.0768, -1.6504,  0.0000,  0.6592]],\n",
       "\n",
       "        [[ 0.1909,  0.4045,  0.1603,  1.4609, -0.1328,  0.9204,  0.0000],\n",
       "         [-0.9092, -0.2815, -2.8887, -0.7837,  1.5059, -0.3010,  0.2515],\n",
       "         [ 1.4805, -0.1616, -0.5483, -0.5347,  2.8027, -0.2749, -0.4602],\n",
       "         [-1.1729, -0.3936,  0.0195,  0.6851,  0.3359,  0.1119, -2.1621],\n",
       "         [-0.2734,  1.3857,  0.3467,  1.3154, -1.3350, -0.6890, -0.4072],\n",
       "         [-0.1079, -1.5117, -0.5781, -0.1903,  0.5835,  0.5146,  1.1484],\n",
       "         [ 1.2051,  0.5527,  1.4395,  0.8159, -1.5176, -0.7070,  1.0127]],\n",
       "\n",
       "        [[ 0.6919,  0.0134,  0.9688,  0.0907, -0.4102, -0.7295,  0.0090],\n",
       "         [-1.6982, -0.9238,  0.3647,  0.1588, -1.0703,  0.4690, -1.0020],\n",
       "         [ 1.2676, -0.9380,  1.0068,  0.4919,  0.5112, -0.3738,  0.0767],\n",
       "         [ 2.0723, -0.5249, -0.0633, -0.2218, -0.6787,  1.5127, -1.7002],\n",
       "         [ 0.5513,  0.1691,  0.8442,  1.3867, -1.4570, -1.3379,  0.8535],\n",
       "         [-2.1504,  0.9253,  1.3096, -1.2549, -0.3340, -0.4724, -0.9995],\n",
       "         [ 0.9419,  1.6143,  0.6265, -1.0410,  0.6909, -0.8438,  0.1174]],\n",
       "\n",
       "        [[-1.5547, -0.4326, -0.3550,  0.4570,  1.5693, -0.4116,  0.0437],\n",
       "         [-0.0938, -1.9971,  1.9502, -2.2891, -0.8970, -0.0848, -0.1060],\n",
       "         [-0.1056, -0.1871,  0.8716, -0.5376, -0.9434,  1.9697, -0.3931],\n",
       "         [ 0.6582, -1.3926, -0.0803,  0.6060, -0.5215,  1.9990, -1.3574],\n",
       "         [ 1.5518,  1.1035, -0.9121,  0.3481, -1.1963,  0.3923,  1.0195],\n",
       "         [-0.0759, -1.4629,  0.7036, -1.3262,  0.3420, -1.1133,  0.7739],\n",
       "         [ 1.1172, -1.2627,  0.2421,  0.4944,  0.7769, -0.2330,  0.8379]],\n",
       "\n",
       "        [[-0.9170,  1.1592,  1.6689, -0.4133, -1.5166, -0.8496, -0.0332],\n",
       "         [-0.4600,  0.0655, -0.3770,  0.3855,  0.2761, -0.0354,  0.6973],\n",
       "         [ 0.5479, -0.5088,  0.8955,  0.1852, -0.1212,  1.9863,  1.2715],\n",
       "         [ 0.3376,  1.0645,  1.6201,  0.9180, -0.8237, -0.0051, -1.0322],\n",
       "         [ 0.4883,  0.6675,  0.7725, -0.2842, -0.3831, -0.3828,  1.0469],\n",
       "         [ 0.3867,  0.3071,  1.4932, -0.1450, -0.3562, -1.1494,  0.8491],\n",
       "         [-0.4871, -0.2095, -0.7065, -0.4429,  0.6357,  0.3003,  0.4500]],\n",
       "\n",
       "        [[-1.8848,  0.2593, -0.8867,  0.3606, -0.2583,  0.5220, -0.4041],\n",
       "         [ 0.1083, -0.1198, -0.7363, -0.6675,  0.2262,  0.1868, -0.6963],\n",
       "         [-1.3506,  1.1729, -0.6826, -0.7202,  0.3472,  0.3997,  1.6348],\n",
       "         [ 1.0156,  0.7812,  0.0000,  1.9277, -0.5835, -0.3330, -0.4438],\n",
       "         [ 0.2932,  0.2032,  0.4373, -1.0029, -0.5630, -1.4854, -1.2510],\n",
       "         [ 0.0302, -0.6250, -0.2534, -1.0254, -0.4810,  0.5698, -1.0732],\n",
       "         [ 1.6123,  0.7109, -0.2524, -0.8979, -0.8843, -0.4104,  1.4619]]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T20:05:20.687923Z",
     "start_time": "2024-05-13T20:05:20.684817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define tensor_b\n",
    "tensor_b = torch.tensor([8, 9, 10])\n",
    "\n",
    "# Clamp the values in tensor_b to be within the indices range of tensor_a\n",
    "# tensor_a.shape gives (21, 21, 21), so the maximum valid index is (20, 20, 20)\n",
    "clamped_b = torch.clamp(tensor_b, 0, tensor_a.shape[0] - 1)\n",
    "print(clamped_b)\n",
    "# Access the element in tensor_a using the clamped indices\n",
    "element = tensor_a[clamped_b[0], clamped_b[1], clamped_b[2]]\n",
    "\n",
    "print(\"Element at clamped index:\", element)\n"
   ],
   "id": "6d7e401aac56d86c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 6])\n",
      "Element at clamped index: tensor(1.4619)\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from random import shuffle\n",
    "\n",
    "def find_next_valid_move(self, agent):\n",
    "    terminate = False\n",
    "    tested_indices = set()  # Track tested indices\n",
    "    indices_pool = list(range(len(self.transitions)))  # List of all possible indices\n",
    "    shuffle(indices_pool)  # Shuffle to randomize the initial order\n",
    "\n",
    "    while not terminate and indices_pool:\n",
    "        idx = indices_pool.pop()  # Select and remove the last element from the pool\n",
    "        tested_indices.add(idx)\n",
    "        next_position = self.transitions[idx] + agent.location[-1]  # Update the position based on the chosen transition\n",
    "\n",
    "        if self.valid_env_move(newpos=next_position, agent=agent) and self.l(mode='hard', locations=next_position, intensity=0.99):\n",
    "            agent.violations.append(self.env.get_cell(next_position))\n",
    "            print(\"OBS.step: Hard Constraint Violation, seeding next valid move\")\n",
    "            terminate = True  # Valid move found, terminate while loop\n",
    "        elif not indices_pool:  # If no indices left, all possibilities have been tested\n",
    "            print(\"No valid moves left to test.\")\n",
    "            terminate = True  # Could also handle reinitialization or error handling here\n",
    "\n",
    "    if not terminate:\n",
    "        # Handle the case where no valid move was found after testing all transitions\n",
    "        print(\"All transitions tested, no valid move found.\")\n",
    "\n",
    "    return terminate\n"
   ],
   "id": "e36ec74bc73ad8e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T02:27:44.584514Z",
     "start_time": "2024-05-15T02:27:44.550874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def l(context, loc, intensity):\n",
    "    # Dummy implementation for the l() function\n",
    "    # This should be replaced with the actual implementation\n",
    "    # Assuming it returns a negative value based on the state's undesirability\n",
    "    if context == 'soft':\n",
    "        #return -intensity * torch.norm(loc)  # Dummy calculation\n",
    "        print(intensity)\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def reward(agent, loc, cell, invalid=False, slip_roll=False, death=False):\n",
    "    energy, risk, wall, death_state, m = cell\n",
    "    end_pos = torch.tensor([9, 9])  # Example end position\n",
    "    penalty = -m + l('soft', loc, intensity=0.99)  # default penalty calculation\n",
    "    \n",
    "    # Apply the penalty adjustments based on conditions\n",
    "    if invalid:\n",
    "        penalty *= 3  # Triple the penalty if the action was invalid\n",
    "    if slip_roll:\n",
    "        penalty *= 1.6  # Increase the penalty by 60% if a slip occurred\n",
    "    if death or death_state:\n",
    "        penalty *= 4  # Quadruple the penalty if death occurred\n",
    "\n",
    "    # Check if the agent reached the end position\n",
    "    if torch.equal(end_pos, loc):\n",
    "        # Reward the agent based on reaching the target and the safety of the path\n",
    "        penalty = -5 * penalty  # Positive reward, 5 times the negated penalty\n",
    "\n",
    "        # Calculate distance to end position and apply a logarithmic scale discount\n",
    "        distance_to_end = torch.norm(end_pos - loc)\n",
    "        #max_distance = 2 * torch.prod(self.env.sz_tensor).item()\n",
    "        max_distance = 2 * torch.prod(torch.tensor([10, 10])).item()\n",
    "        discount_factor = torch.log1p(distance_to_end / max_distance)\n",
    "        discount = torch.clamp(discount_factor, max=0.5)\n",
    "        penalty *= (1 - discount)  # Apply discount to the reward\n",
    "\n",
    "    return penalty\n",
    "\n",
    "# Example usage:\n",
    "# Define the environment size and agent position\n",
    "env_sz_tensor = torch.tensor([10, 10])  # Example environment size\n",
    "agent = end_pos = torch.tensor([9, 9])\n",
    "loc = torch.tensor([9, 9])  # Location of the agent\n",
    "cell = (0.5, 0.1, 0, False, 0.2)  # Example cell properties (energy, risk, wall, death, map value)\n",
    "\n",
    "# Call the reward function\n",
    "reward_val = reward(agent, loc, cell, invalid=False, slip_roll=True, death=False)\n",
    "print(f\"Reward: {reward_val}\")\n"
   ],
   "id": "61deeec4df5aa20a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 49\u001B[0m\n\u001B[1;32m     46\u001B[0m cell \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m0.2\u001B[39m)  \u001B[38;5;66;03m# Example cell properties (energy, risk, wall, death, map value)\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Call the reward function\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m reward_val \u001B[38;5;241m=\u001B[39m reward(agent, loc, cell, invalid\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, slip_roll\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, death\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReward: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreward_val\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 32\u001B[0m, in \u001B[0;36mreward\u001B[0;34m(agent, loc, cell, invalid, slip_roll, death)\u001B[0m\n\u001B[1;32m     29\u001B[0m penalty \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m penalty  \u001B[38;5;66;03m# Positive reward, 5 times the negated penalty\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Calculate distance to end position and apply a logarithmic scale discount\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m distance_to_end \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnorm(end_pos \u001B[38;5;241m-\u001B[39m loc)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m#max_distance = 2 * torch.prod(self.env.sz_tensor).item()\u001B[39;00m\n\u001B[1;32m     34\u001B[0m max_distance \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mprod(torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m]))\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Zeolite/lib/python3.11/site-packages/torch/functional.py:1615\u001B[0m, in \u001B[0;36mnorm\u001B[0;34m(input, p, dim, keepdim, out, dtype)\u001B[0m\n\u001B[1;32m   1613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfro\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m (dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dim, (\u001B[38;5;28mint\u001B[39m, torch\u001B[38;5;241m.\u001B[39mSymInt)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(dim) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m   1614\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1615\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mvector_norm(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m2\u001B[39m, _dim, keepdim, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   1616\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1617\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mvector_norm(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m2\u001B[39m, _dim, keepdim, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T02:50:18.787281Z",
     "start_time": "2024-05-15T02:50:18.782629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def step(self, agent, action, safety=True, is_slip_activated=True):\n",
    "    \"\"\"Performs a step in the environment based on the agent's action.\"\"\"\n",
    "    penalty = 0\n",
    "    terminate = False\n",
    "    roll = False  # Default roll value\n",
    "\n",
    "    # Calculate next position based on the action\n",
    "    next_position = self.transitions[action] + torch.as_tensor(agent.location[-1])\n",
    "    valid = self.valid_env_move(newpos=next_position, agent=agent)\n",
    "\n",
    "    if not valid:\n",
    "        agent.violations.append((self.env.get_cell(next_position), next_position))\n",
    "        print(\"OBS.step: Invalid Move\")\n",
    "        terminate = True\n",
    "        return (\n",
    "            self.reward(agent=agent, loc=next_position, cell=self.env.get_cell(next_position), invalid=True),\n",
    "            self.observation(next_position, agent.end, agent.team),\n",
    "            next_position, terminate\n",
    "        )\n",
    "\n",
    "    # Apply slip effect if activated\n",
    "    if is_slip_activated:\n",
    "        roll, risk = self.env.roll(next_position)\n",
    "        if roll:\n",
    "            old_position = next_position\n",
    "            next_position = self.transitions[np.random.randint(len(self.transitions))] + torch.as_tensor(agent.location[-1])\n",
    "            valid = self.valid_env_move(newpos=next_position, agent=agent)\n",
    "\n",
    "            if not valid:\n",
    "                agent.violations.append((self.env.get_cell(next_position), next_position))\n",
    "                print(\"OBS.step.slipped: Invalid Move\")\n",
    "                terminate = True\n",
    "                return (\n",
    "                    self.reward(agent=agent, loc=next_position, cell=self.env.get_cell(next_position), invalid=True),\n",
    "                    self.observation(next_position, agent.end, agent.team),\n",
    "                    next_position, terminate\n",
    "                )\n",
    "\n",
    "    # Enforce hard safety constraints if safety is enabled\n",
    "    if safety and self.l(mode='hard', locations=next_position, intensity=0.99):\n",
    "        agent.violations.append((self.env.get_cell(next_position), next_position))\n",
    "        print(\"OBS.step: Hard Constraint Violation\")\n",
    "        terminate = True  # Terminate if hard constraints are violated\n",
    "        next_position = self.find_safe_position(agent)\n",
    "\n",
    "    reward = self.reward(agent, next_position, self.env.get_cell(next_position), invalid=terminate, slip_roll=roll)\n",
    "    next_observation = self.observation(next_position, agent.end, agent.team)\n",
    "    print(\"OBS.step.reward: \", reward)\n",
    "\n",
    "    return reward, next_observation, next_position, terminate\n",
    "\n",
    "def find_safe_position(self, agent):\n",
    "    \"\"\"Finds a safe position starting from the last known valid position of the agent.\"\"\"\n",
    "    for index in np.random.permutation(len(self.transitions)):\n",
    "        potential_pos = self.transitions[index] + torch.as_tensor(agent.location[-1])\n",
    "        if self.valid_env_move(newpos=potential_pos, agent=agent) and not self.l(mode='hard', locations=potential_pos, intensity=0.99):\n",
    "            print(f\"Valid move found at index {index}, position {potential_pos}\")\n",
    "            return potential_pos\n",
    "\n",
    "    print(\"OBS.step.l.seed: No valid moves found.\")\n",
    "    return agent.location[-1]  # Return to the last valid position if no valid move is found\n"
   ],
   "id": "b5fe2d78daaf69a4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d3cb1944ea8f69e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
